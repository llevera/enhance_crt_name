{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1715729329963
        }
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import wfdb\n",
        "from collections import Counter\n",
        "import pickle\n",
        "import random\n",
        "import sys\n",
        "from tqdm import tqdm\n",
        "from scipy.interpolate import interp1d\n",
        "\n",
        "label_group_map = {'N':'N', 'L':'N', 'R':'N', 'V':'V', '/':'Q', '!':'V', 'A':'S', 'F':'F', 'f':'Q', 'j':'N', 'a':'S', 'E':'V', 'J':'S', 'e':'N', 'Q':'Q', 'S':'S', '[':'V',']':'V'}\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    path = 'mit-bih-arrhythmia-database-1.0.0'\n",
        "    save_path = 'data/'\n",
        "    primary_lead = 'MLII'\n",
        "    secondary_leads = ['V1', 'V2', 'V4', 'V5']\n",
        "    num_samples = 200  # Number of samples to extract\n",
        "\n",
        "    all_data = []\n",
        "    all_data_single_lead = []\n",
        "    all_group = []\n",
        "\n",
        "    with open(os.path.join(path, 'RECORDS'), 'r') as fin:\n",
        "        all_record_name = fin.read().strip().split('\\n')\n",
        "\n",
        "    for record_name in all_record_name:\n",
        "        try:\n",
        "            tmp_ann_res = wfdb.rdann(path + '/' + record_name, 'atr').__dict__\n",
        "            tmp_data_res = wfdb.rdsamp(path + '/' + record_name)\n",
        "        except:\n",
        "            print('read data failed')\n",
        "            continue\n",
        "        fs = tmp_data_res[1]['fs']\n",
        "        half_samples = num_samples // 2\n",
        "\n",
        "        lead_in_data = tmp_data_res[1]['sig_name']\n",
        "        if primary_lead in lead_in_data:\n",
        "            primary_channel = lead_in_data.index(primary_lead)\n",
        "            primary_data = tmp_data_res[0][:, primary_channel]\n",
        "\n",
        "            for secondary_lead in secondary_leads:\n",
        "                if secondary_lead in lead_in_data:\n",
        "                    secondary_channel = lead_in_data.index(secondary_lead)\n",
        "                    secondary_data = tmp_data_res[0][:, secondary_channel]\n",
        "\n",
        "                    idx_list = list(tmp_ann_res['sample'])\n",
        "                    label_list = tmp_ann_res['symbol']\n",
        "                    for i in range(len(label_list)):\n",
        "                        s = label_list[i]\n",
        "                        if s in label_group_map.keys():\n",
        "                            idx_start = idx_list[i] - half_samples\n",
        "                            idx_end = idx_list[i] + half_samples\n",
        "                            if idx_start < 0 or idx_end > len(primary_data):\n",
        "                                continue\n",
        "                            else:\n",
        "                                primary_segment = primary_data[idx_start:idx_end]\n",
        "                                secondary_segment = secondary_data[idx_start:idx_end]\n",
        "\n",
        "                                combined_data = np.vstack((primary_segment, secondary_segment))\n",
        "                                combined_data = np.swapaxes(combined_data, 0, 1)\n",
        "                                all_data.append(combined_data)\n",
        "\n",
        "                                single_combined = np.vstack(primary_segment)\n",
        "                                all_data_single_lead.append(single_combined)\n",
        "                                all_group.append(label_group_map[s])\n",
        "                    print('record_name:{}, leads:{}/{}'.format(record_name, primary_lead, secondary_lead))\n",
        "        else:\n",
        "            print('lead in data: [{}]. primary lead {} not found in {}'.format(lead_in_data, primary_lead, record_name))\n",
        "            continue\n",
        "\n",
        "    all_data = np.array(all_data)\n",
        "    all_group = np.array(all_group)\n",
        "    all_data_single_lead = np.array(all_data_single_lead)\n",
        "    print(all_data.shape)\n",
        "    print(all_data_single_lead.shape)\n",
        "    print(Counter(all_group))\n",
        "    np.save(os.path.join(save_path, 'mitdb_data.npy'), all_data)\n",
        "    np.save(os.path.join(save_path, 'mitdb_data_single_lead.npy'), all_data_single_lead)\n",
        "    np.save(os.path.join(save_path, 'mitdb_group.npy'), all_group)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "gather": {
          "logged": 1715754947170
        }
      },
      "outputs": [],
      "source": [
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras import utils\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import datetime\n",
        "import numpy as np\n",
        "from src import train_and_evaluate\n",
        "from importlib import reload\n",
        "reload(train_and_evaluate)\n",
        "from src import crtnet_models\n",
        "reload(crtnet_models)\n",
        "\n",
        "def label2index(i):\n",
        "    m = {'N':0, 'S':1, 'V':2, 'F':3, 'Q':4}\n",
        "    return m[i]\n",
        "\n",
        "def load_and_preprocess_data(path, num_classes):\n",
        "    data = np.load(os.path.join(path, 'mitdb_data.npy'))\n",
        "    label_str = np.load(os.path.join(path, 'mitdb_group.npy'))\n",
        "    label = np.array([label2index(i) for i in label_str])\n",
        "    one_hot = utils.to_categorical(label, num_classes=num_classes)\n",
        "    return data, one_hot\n",
        "\n",
        "path = 'data/'\n",
        "num_classes = 5\n",
        "class_names = ['N', 'S', 'V', 'F', 'Q']  # Update based on your classes\n",
        "\n",
        "samples, one_hot_encoding_labels = load_and_preprocess_data(path, num_classes)\n",
        "\n",
        "stopping = keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True)\n",
        "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, min_lr=0.00001)\n",
        "\n",
        "model_methods = [\n",
        "    crtnet_models.create_crtnet_alternate_vgg1,\n",
        "   # crtnet_models.create_crtnet_original_vgg1\n",
        "]\n",
        "\n",
        "for create_crtnet_method in model_methods:\n",
        "    train_and_evaluate.train_and_evaluate_model(\n",
        "        create_crtnet_method,\n",
        "        samples=samples,\n",
        "        one_hot_encoding_labels=one_hot_encoding_labels,\n",
        "        callbacks=[reduce_lr, stopping],\n",
        "        is_multilabel=False,\n",
        "        epochs=100,\n",
        "        folds=None,\n",
        "        batch_size=128,\n",
        "        classes=class_names,\n",
        "        initial_learning_rate=0.0001, \n",
        "        number_of_leads=samples.shape[2]\n",
        "    )\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "tf215gpu"
    },
    "kernelspec": {
      "display_name": "tf215gpu",
      "language": "python",
      "name": "tf215gpu"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
