{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import wfdb\n",
        "from collections import Counter\n",
        "import pickle\n",
        "import random\n",
        "import sys\n",
        "from tqdm import tqdm\n",
        "\n",
        "label_group_map = {'N':'N', 'L':'N', 'R':'N', 'V':'V', '/':'Q', 'A':'S', 'F':'F', 'f':'Q', 'j':'S', 'a':'S', 'E':'V', 'J':'S', 'e':'S', 'Q':'Q', 'S':'S'}\n",
        "\n",
        "def resample_unequal(ts, fs_in, fs_out):\n",
        "    \"\"\"\n",
        "    interploration\n",
        "    \"\"\"\n",
        "    fs_in, fs_out = int(fs_in), int(fs_out)\n",
        "    if fs_out == fs_in:\n",
        "        return ts\n",
        "    else:\n",
        "        x_old = np.linspace(0, 1, num=fs_in, endpoint=True)\n",
        "        x_new = np.linspace(0, 1, num=fs_out, endpoint=True)\n",
        "        y_old = ts\n",
        "        f = interp1d(x_old, y_old, kind='linear')\n",
        "        y_new = f(x_new)\n",
        "        return y_new\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    path = 'mit-bih-arrhythmia-database-1.0.0'\n",
        "    save_path = 'data/'\n",
        "    # valid_lead = ['MLII', 'II', 'I', 'MLI', 'V5'] \n",
        "    valid_lead = ['MLII'] \n",
        "    fs_out = 360\n",
        "    test_ratio = 0.2\n",
        "\n",
        "    train_ind = []\n",
        "    test_ind = []\n",
        "    all_pid = []\n",
        "    all_data = []\n",
        "    all_label = []\n",
        "    all_group = []\n",
        "\n",
        "    with open(os.path.join(path, 'RECORDS'), 'r') as fin:\n",
        "        all_record_name = fin.read().strip().split('\\n')\n",
        "    test_pid = random.choices(all_record_name, k=int(len(all_record_name)*test_ratio))\n",
        "    train_pid = list(set(all_record_name) - set(test_pid))\n",
        "\n",
        "    for record_name in all_record_name:\n",
        "        try:\n",
        "            tmp_ann_res = wfdb.rdann(path + '/' + record_name, 'atr').__dict__\n",
        "            tmp_data_res = wfdb.rdsamp(path + '/' + record_name)\n",
        "        except:\n",
        "            print('read data failed')\n",
        "            continue\n",
        "        fs = tmp_data_res[1]['fs']\n",
        "        ## total 1 second for each\n",
        "        left_offset = int(1.0*fs / 2)\n",
        "        right_offset = int(fs) - int(1.0*fs / 2)\n",
        "\n",
        "        lead_in_data = tmp_data_res[1]['sig_name']\n",
        "        my_lead_all = []\n",
        "        for tmp_lead in valid_lead:\n",
        "            if tmp_lead in lead_in_data:\n",
        "                my_lead_all.append(tmp_lead)\n",
        "        if len(my_lead_all) != 0:\n",
        "            for my_lead in my_lead_all:\n",
        "                channel = lead_in_data.index(my_lead)\n",
        "                tmp_data = tmp_data_res[0][:, channel]\n",
        "\n",
        "                idx_list = list(tmp_ann_res['sample'])\n",
        "                label_list = tmp_ann_res['symbol']\n",
        "                for i in range(len(label_list)):\n",
        "                    s = label_list[i]\n",
        "                    if s in label_group_map.keys():\n",
        "                        idx_start = idx_list[i]-left_offset\n",
        "                        idx_end = idx_list[i]+right_offset\n",
        "                        if idx_start < 0 or idx_end > len(tmp_data):\n",
        "                            continue\n",
        "                        else:\n",
        "                            all_pid.append(record_name)\n",
        "                            all_data.append(resample_unequal(tmp_data[idx_start:idx_end], fs, fs_out))\n",
        "                            all_label.append(s)\n",
        "                            all_group.append(label_group_map[s])\n",
        "                            if record_name in train_pid:\n",
        "                                train_ind.append(True)\n",
        "                                test_ind.append(False)\n",
        "                            else:\n",
        "                                train_ind.append(False)\n",
        "                                test_ind.append(True)\n",
        "                    else:\n",
        "                        continue\n",
        "                print('record_name:{}, lead:{}, fs:{}, cumcount: {}'.format(record_name, my_lead, fs, len(all_pid)))\n",
        "        else:\n",
        "            print('lead in data: [{0}]. no valid lead in {1}'.format(lead_in_data, record_name))\n",
        "            continue\n",
        "\n",
        "    all_pid = np.array(all_pid)\n",
        "    all_data = np.array(all_data)\n",
        "    all_label = np.array(all_label)\n",
        "    all_group = np.array(all_group)\n",
        "    train_ind = np.array(train_ind)\n",
        "    test_ind = np.array(test_ind)\n",
        "    print(all_data.shape)\n",
        "    print(all_label.shape, np.sum(train_ind), np.sum(test_ind))\n",
        "    print(Counter(all_label))\n",
        "    print(Counter(all_group))\n",
        "    print(Counter(all_group[train_ind]), Counter(all_group[test_ind]))\n",
        "    np.save(os.path.join(save_path, 'mitdb_data.npy'), all_data)\n",
        "    np.save(os.path.join(save_path, 'mitdb_label.npy'), all_label)\n",
        "    np.save(os.path.join(save_path, 'mitdb_group.npy'), all_group)\n",
        "    np.save(os.path.join(save_path, 'mitdb_pid.npy'), all_pid)\n",
        "    np.save(os.path.join(save_path, 'mitdb_train_ind.npy'), train_ind)\n",
        "    np.save(os.path.join(save_path, 'mitdb_test_ind.npy'), test_ind)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1715159381519
        }
      },
      "id": "30fe23b5"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn import preprocessing\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from tensorflow.keras import layers, models, callbacks, utils\n",
        "import keras_nlp as nlp\n",
        "import keras as keras\n",
        "import tensorflow as tf\n",
        "\n",
        "# Check TensorFlow version\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "\n",
        "# List all physical devices visible to TensorFlow\n",
        "devices = tf.config.list_physical_devices()\n",
        "print(\"Devices:\", devices)\n",
        "\n",
        "# Specifically check for GPU devices\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    print(\"GPU is available.\")\n",
        "    try:\n",
        "        # Currently, memory growth needs to be the same across GPUs\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        logical_gpus = tf.config.list_logical_devices('GPU')\n",
        "        print(f\"{len(gpus)} Physical GPUs, {len(logical_gpus)} Logical GPUs\")\n",
        "    except RuntimeError as e:\n",
        "        # Memory growth must be set before GPUs have been initialized\n",
        "        print(e)\n",
        "else:\n",
        "    print(\"GPU is not available.\")\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"CUDA version:\", tf.sysconfig.get_build_info()[\"cuda_version\"])\n",
        "\n",
        "def plot_confusion_matrix(cm, class_names):\n",
        "    \"\"\"\n",
        "    Returns a matplotlib figure containing the plotted confusion matrix.\n",
        "    \"\"\"\n",
        "    figure = plt.figure(figsize=(8, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.title('Confusion matrix')\n",
        "    plt.ylabel('Actual label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    return figure\n",
        "\n",
        "def train_and_evaluate_model(model, train_x, validation_x, train_y, validation_y, class_names, callbacks=None, epochs=20, batch_size=64):\n",
        "    \n",
        "    history = model.fit(train_x, train_y, epochs=epochs, batch_size=batch_size, validation_data=(validation_x, validation_y), callbacks=callbacks)\n",
        "\n",
        "    pd.DataFrame(history.history).plot(\n",
        "        figsize=(8, 5), xlim=[0, epochs], ylim=[0, 1], grid=True, xlabel=\"Epoch\",\n",
        "        style=[\"r--\", \"r--.\", \"b-\", \"b-*\"])\n",
        "    plt.legend(loc=\"lower left\")\n",
        "    plt.show()\n",
        "\n",
        "    # Predictions for confusion matrix and classification report\n",
        "    Y_pred = model.predict(X_test)\n",
        "    Y_pred_classes = np.argmax(Y_pred, axis=1)\n",
        "    Y_true_classes = np.argmax(Y_test, axis=1)\n",
        "\n",
        "    # Classification report\n",
        "    print(classification_report(Y_true_classes, Y_pred_classes, target_names=class_names))\n",
        "\n",
        "    # Confusion matrix\n",
        "    cm = confusion_matrix(Y_true_classes, Y_pred_classes)\n",
        "    fig = plot_confusion_matrix(cm, class_names)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def label2index(i):\n",
        "    m = {'N':0, 'S':1, 'V':2, 'F':3, 'Q':4}\n",
        "    return m[i]\n",
        "\n",
        "\n",
        "def vgg_block(input, cnn_units):\n",
        "    output = input\n",
        "    output = layers.Conv1D(cnn_units, 3, padding='same', activation='relu')(output)\n",
        "    output = layers.BatchNormalization()(output)\n",
        "    output = layers.Dropout(0.1)(output)\n",
        "    output = layers.Conv1D(cnn_units, 3, padding='same', activation='relu')(output)\n",
        "    output = layers.BatchNormalization()(output)\n",
        "    output = layers.Dropout(0.1)(output)\n",
        "    output = layers.Conv1D(cnn_units, 24, padding='same', activation='relu', strides=2)(output)\n",
        "    output = layers.BatchNormalization()(output)\n",
        "    output = layers.MaxPooling1D(2, padding='same')(output)\n",
        "    return output\n",
        "\n",
        "def create_crt_baseline(number_of_leads=None, cnn_units=128, vgg_blocks=1, rnn_units=64,\n",
        "                   transformer_encoders=4, att_dim=64, att_heads=8, fnn_units=64, num_classes=5):\n",
        "    input = layers.Input(shape=(None, number_of_leads))\n",
        "    output = input\n",
        "\n",
        "    for _ in range(vgg_blocks):\n",
        "        output = layers.BatchNormalization()(output)\n",
        "        output = vgg_block(output, cnn_units)\n",
        "\n",
        "    output = layers.BatchNormalization()(output)\n",
        "    output = layers.Bidirectional(layers.GRU(rnn_units, return_sequences=True), merge_mode='sum')(output)\n",
        "    output = layers.BatchNormalization()(output)\n",
        "\n",
        "    if transformer_encoders > 0:\n",
        "        output = output + nlp.layers.SinePositionEncoding(max_wavelength=10000)(output)\n",
        "\n",
        "        for _ in range(transformer_encoders):\n",
        "            output = layers.BatchNormalization()(output)\n",
        "            output = nlp.layers.TransformerEncoder(att_dim, att_heads)(output)\n",
        "\n",
        "        output = layers.GlobalAveragePooling1D()(output)\n",
        "\n",
        "    output = layers.Dropout(0.2)(output)\n",
        "    output = layers.Dense(fnn_units, activation='relu')(output)\n",
        "    output = layers.BatchNormalization()(output)\n",
        "    output = layers.Dropout(0.2)(output)\n",
        "    output = layers.Dense(fnn_units, activation='relu')(output)\n",
        "    output = layers.BatchNormalization()(output)\n",
        "    output = layers.Dense(num_classes, activation='softmax')(output)\n",
        "    model = keras.models.Model(input, output)\n",
        "    return model\n",
        "\n",
        "def load_and_preprocess_data(path, num_classes):\n",
        "    data = np.load(os.path.join(path, 'mitdb_data.npy'))\n",
        "    label_str = np.load(os.path.join(path, 'mitdb_group.npy'))\n",
        "    label = np.array([label2index(i) for i in label_str])\n",
        "\n",
        "    train_ind = np.load(os.path.join(path, 'mitdb_train_ind.npy'))\n",
        "    test_ind = np.load(os.path.join(path, 'mitdb_test_ind.npy'))\n",
        "    data = preprocessing.scale(data, axis=1)\n",
        "    X_train = data[train_ind]\n",
        "    X_test = data[test_ind]\n",
        "    Y_train = utils.to_categorical(label[train_ind], num_classes=num_classes)\n",
        "    Y_test = utils.to_categorical(label[test_ind], num_classes=num_classes)\n",
        "\n",
        "    ros = RandomOverSampler(random_state=0)\n",
        "    X_train, Y_train = ros.fit_resample(X_train, Y_train)\n",
        "\n",
        "    return X_train, X_test, Y_train, Y_test"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1715159387065
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "e70a1d9d-35d5-4c0c-95dd-8db2faa18120"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "from sklearn import preprocessing\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from tensorflow.keras import layers, models, callbacks, utils\n",
        "import keras_nlp as nlp\n",
        "from tensorflow.keras.layers import LayerNormalization, Dense, Dropout, Add, Conv1D, MaxPooling1D, BatchNormalization, ReLU, Input, MultiHeadAttention\n",
        "import tensorflow as tf\n",
        "\n",
        "# Define the VGG block\n",
        "def vgg_block(input_tensor, filters):\n",
        "    x = Conv1D(filters, 3, padding='same', activation='relu')(input_tensor)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = ReLU()(x)\n",
        "    x = Conv1D(filters, 3, padding='same', activation='relu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = ReLU()(x)\n",
        "    x = MaxPooling1D(2, padding='same')(x)\n",
        "    return x\n",
        "\n",
        "# Define the Transformer encoder block\n",
        "def transformer_encoder_block(inputs, att_dim, att_heads, dropout_rate, ff_dim):\n",
        "    norm_input = LayerNormalization()(inputs)\n",
        "    attention_output = MultiHeadAttention(num_heads=att_heads, key_dim=att_dim)(norm_input, norm_input)\n",
        "    attention_output = Dropout(dropout_rate)(attention_output)\n",
        "    attention_output = Add()([inputs, attention_output])\n",
        "    attention_output = LayerNormalization()(attention_output)\n",
        "\n",
        "    ff_output = Dense(ff_dim, activation='relu')(attention_output)\n",
        "    ff_output = Dense(inputs.shape[-1])(ff_output)\n",
        "    ff_output = Dropout(dropout_rate)(ff_output)\n",
        "    ff_output = Add()([attention_output, ff_output])\n",
        "    return LayerNormalization()(ff_output)\n",
        "\n",
        "# Define the Bottleneck block\n",
        "def bottleneck_block(x, in_channels, out_channels, kernel_size, stride, downsample, use_bn, use_do):\n",
        "    identity = x\n",
        "\n",
        "    x = Conv1D(filters=out_channels, kernel_size=kernel_size, strides=stride, padding='same')(x)\n",
        "    if use_bn:\n",
        "        x = BatchNormalization()(x)\n",
        "    x = ReLU()(x)\n",
        "    if use_do:\n",
        "        x = Dropout(0.5)(x)\n",
        "\n",
        "    x = Conv1D(filters=out_channels, kernel_size=kernel_size, strides=1, padding='same')(x)\n",
        "    if use_bn:\n",
        "        x = BatchNormalization()(x)\n",
        "    x = ReLU()(x)\n",
        "    if use_do:\n",
        "        x = Dropout(0.5)(x)\n",
        "\n",
        "    if downsample:\n",
        "        identity = MaxPooling1D(pool_size=stride, padding='same')(identity)\n",
        "\n",
        "    if out_channels != in_channels:\n",
        "        identity = Conv1D(filters=out_channels, kernel_size=1, padding='same')(identity)\n",
        "\n",
        "    x = layers.add([x, identity])\n",
        "    return x\n",
        "\n",
        "# Model creation function\n",
        "def create_crtnet_bottleneck(number_of_leads=1,\n",
        "                   cnn_units=128,\n",
        "                   vgg_blocks=1,\n",
        "                   rnn_units=64,\n",
        "                   transformer_encoders=4,\n",
        "                   att_dim=64,\n",
        "                   att_heads=8,\n",
        "                   ff_dim=64,\n",
        "                   dropout_rate=0.1,\n",
        "                   num_classes=5):\n",
        "    input = Input(shape=(None, number_of_leads))\n",
        "    x = input\n",
        "\n",
        "    for _ in range(vgg_blocks):\n",
        "        x = vgg_block(x, cnn_units)\n",
        "\n",
        "    # Add bottleneck blocks\n",
        "    x = bottleneck_block(x, in_channels=cnn_units, out_channels=cnn_units, kernel_size=3, stride=2, downsample=True, use_bn=True, use_do=True)\n",
        "\n",
        "    x = layers.Bidirectional(layers.GRU(rnn_units, return_sequences=True), merge_mode='sum')(x)\n",
        "\n",
        "    for _ in range(transformer_encoders):\n",
        "        x = transformer_encoder_block(x, att_dim, att_heads, dropout_rate, ff_dim)\n",
        "\n",
        "    x = layers.GlobalAveragePooling1D()(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "    x = Dense(ff_dim, activation='relu')(x)\n",
        "    x = Dense(ff_dim, activation='relu')(x)\n",
        "    x = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    model = models.Model(inputs=input, outputs=x)\n",
        "    return model"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "f5755fe7-3e06-4200-9729-531dbe3df411"
    },
    {
      "cell_type": "code",
      "source": [
        "path = 'data/'\n",
        "num_classes = 5\n",
        "class_names = ['N', 'S', 'V', 'F', 'Q']  # Update based on your classes\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = load_and_preprocess_data(path, num_classes)\n",
        "\n",
        "model = create_crt_baseline(number_of_leads=1, num_classes=num_classes)\n",
        "model.summary()\n",
        "\n",
        "# Define callbacks\n",
        "stopping = callbacks.EarlyStopping(patience=5)\n",
        "reduce_lr = callbacks.ReduceLROnPlateau(factor=0.1, patience=3, min_lr=0.001*0.001)\n",
        "\n",
        "initial_learning_rate = 0.0001\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=initial_learning_rate)\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "train_and_evaluate_model(model, X_train, X_test, Y_train, Y_test, class_names=class_names, epochs=20, batch_size=64)\n",
        "\n",
        "    "
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1715159430711
        }
      },
      "id": "5cabca4f"
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "tf215gpu"
    },
    "kernelspec": {
      "name": "tf215gpu",
      "language": "python",
      "display_name": "tf215gpu"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}