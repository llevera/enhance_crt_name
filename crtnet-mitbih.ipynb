{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30fe23b5",
      "metadata": {
        "gather": {
          "logged": 1715159381519
        }
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import wfdb\n",
        "from collections import Counter\n",
        "import pickle\n",
        "import random\n",
        "import sys\n",
        "from tqdm import tqdm\n",
        "\n",
        "label_group_map = {'N':'N', 'L':'N', 'R':'N', 'V':'V', '/':'Q', 'A':'S', 'F':'F', 'f':'Q', 'j':'S', 'a':'S', 'E':'V', 'J':'S', 'e':'S', 'Q':'Q', 'S':'S'}\n",
        "\n",
        "def resample_unequal(ts, fs_in, fs_out):\n",
        "    \"\"\"\n",
        "    interploration\n",
        "    \"\"\"\n",
        "    fs_in, fs_out = int(fs_in), int(fs_out)\n",
        "    if fs_out == fs_in:\n",
        "        return ts\n",
        "    else:\n",
        "        x_old = np.linspace(0, 1, num=fs_in, endpoint=True)\n",
        "        x_new = np.linspace(0, 1, num=fs_out, endpoint=True)\n",
        "        y_old = ts\n",
        "        f = interp1d(x_old, y_old, kind='linear')\n",
        "        y_new = f(x_new)\n",
        "        return y_new\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    path = 'mit-bih-arrhythmia-database-1.0.0'\n",
        "    save_path = 'data/'\n",
        "    # valid_lead = ['MLII', 'II', 'I', 'MLI', 'V5'] \n",
        "    valid_lead = ['MLII'] \n",
        "    fs_out = 360\n",
        "    test_ratio = 0.2\n",
        "\n",
        "    train_ind = []\n",
        "    test_ind = []\n",
        "    all_pid = []\n",
        "    all_data = []\n",
        "    all_label = []\n",
        "    all_group = []\n",
        "\n",
        "    with open(os.path.join(path, 'RECORDS'), 'r') as fin:\n",
        "        all_record_name = fin.read().strip().split('\\n')\n",
        "    test_pid = random.choices(all_record_name, k=int(len(all_record_name)*test_ratio))\n",
        "    train_pid = list(set(all_record_name) - set(test_pid))\n",
        "\n",
        "    for record_name in all_record_name:\n",
        "        try:\n",
        "            tmp_ann_res = wfdb.rdann(path + '/' + record_name, 'atr').__dict__\n",
        "            tmp_data_res = wfdb.rdsamp(path + '/' + record_name)\n",
        "        except:\n",
        "            print('read data failed')\n",
        "            continue\n",
        "        fs = tmp_data_res[1]['fs']\n",
        "        ## total 1 second for each\n",
        "        left_offset = int(1.0*fs / 2)\n",
        "        right_offset = int(fs) - int(1.0*fs / 2)\n",
        "\n",
        "        lead_in_data = tmp_data_res[1]['sig_name']\n",
        "        my_lead_all = []\n",
        "        for tmp_lead in valid_lead:\n",
        "            if tmp_lead in lead_in_data:\n",
        "                my_lead_all.append(tmp_lead)\n",
        "        if len(my_lead_all) != 0:\n",
        "            for my_lead in my_lead_all:\n",
        "                channel = lead_in_data.index(my_lead)\n",
        "                tmp_data = tmp_data_res[0][:, channel]\n",
        "\n",
        "                idx_list = list(tmp_ann_res['sample'])\n",
        "                label_list = tmp_ann_res['symbol']\n",
        "                for i in range(len(label_list)):\n",
        "                    s = label_list[i]\n",
        "                    if s in label_group_map.keys():\n",
        "                        idx_start = idx_list[i]-left_offset\n",
        "                        idx_end = idx_list[i]+right_offset\n",
        "                        if idx_start < 0 or idx_end > len(tmp_data):\n",
        "                            continue\n",
        "                        else:\n",
        "                            all_pid.append(record_name)\n",
        "                            all_data.append(resample_unequal(tmp_data[idx_start:idx_end], fs, fs_out))\n",
        "                            all_label.append(s)\n",
        "                            all_group.append(label_group_map[s])\n",
        "                            if record_name in train_pid:\n",
        "                                train_ind.append(True)\n",
        "                                test_ind.append(False)\n",
        "                            else:\n",
        "                                train_ind.append(False)\n",
        "                                test_ind.append(True)\n",
        "                    else:\n",
        "                        continue\n",
        "                print('record_name:{}, lead:{}, fs:{}, cumcount: {}'.format(record_name, my_lead, fs, len(all_pid)))\n",
        "        else:\n",
        "            print('lead in data: [{0}]. no valid lead in {1}'.format(lead_in_data, record_name))\n",
        "            continue\n",
        "\n",
        "    all_pid = np.array(all_pid)\n",
        "    all_data = np.array(all_data)\n",
        "    all_label = np.array(all_label)\n",
        "    all_group = np.array(all_group)\n",
        "    train_ind = np.array(train_ind)\n",
        "    test_ind = np.array(test_ind)\n",
        "    print(all_data.shape)\n",
        "    print(all_label.shape, np.sum(train_ind), np.sum(test_ind))\n",
        "    print(Counter(all_label))\n",
        "    print(Counter(all_group))\n",
        "    print(Counter(all_group[train_ind]), Counter(all_group[test_ind]))\n",
        "    np.save(os.path.join(save_path, 'mitdb_data.npy'), all_data)\n",
        "    np.save(os.path.join(save_path, 'mitdb_label.npy'), all_label)\n",
        "    np.save(os.path.join(save_path, 'mitdb_group.npy'), all_group)\n",
        "    np.save(os.path.join(save_path, 'mitdb_pid.npy'), all_pid)\n",
        "    np.save(os.path.join(save_path, 'mitdb_train_ind.npy'), train_ind)\n",
        "    np.save(os.path.join(save_path, 'mitdb_test_ind.npy'), test_ind)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e70a1d9d-35d5-4c0c-95dd-8db2faa18120",
      "metadata": {
        "gather": {
          "logged": 1715409797875
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import keras_nlp as nlp \n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras import layers, models, callbacks, utils\n",
        "from sklearn.metrics import classification_report, multilabel_confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from numpy import argmax\n",
        "import tensorflow as tf\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from importlib import reload\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from src import crtnet_models\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn import preprocessing\n",
        "\n",
        "# Check TensorFlow version\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "\n",
        "# List all physical devices visible to TensorFlow\n",
        "devices = tf.config.list_physical_devices()\n",
        "print(\"Devices:\", devices)\n",
        "\n",
        "# Specifically check for GPU devices\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    print(\"GPU is available.\")\n",
        "    try:\n",
        "        # Currently, memory growth needs to be the same across GPUs\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        logical_gpus = tf.config.list_logical_devices('GPU')\n",
        "        print(f\"{len(gpus)} Physical GPUs, {len(logical_gpus)} Logical GPUs\")\n",
        "    except RuntimeError as e:\n",
        "        # Memory growth must be set before GPUs have been initialized\n",
        "        print(e)\n",
        "else:\n",
        "    print(\"GPU is not available.\")\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"CUDA version:\", tf.sysconfig.get_build_info()[\"cuda_version\"])\n",
        "\n",
        "def plot_confusion_matrix(cm, class_names):\n",
        "    \"\"\"\n",
        "    Returns a matplotlib figure containing the plotted confusion matrix.\n",
        "    \"\"\"\n",
        "    figure = plt.figure(figsize=(8, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.title('Confusion matrix')\n",
        "    plt.ylabel('Actual label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    return figure\n",
        "\n",
        "def train_and_evaluate_model(model, train_x, validation_x, train_y, validation_y, class_names, callbacks=None, epochs=20, batch_size=64, style=None):\n",
        "    \n",
        "    history = model.fit(train_x, train_y, epochs=epochs, batch_size=batch_size, validation_data=(validation_x, validation_y), callbacks=callbacks)\n",
        "\n",
        "    pd.DataFrame(history.history).plot(\n",
        "        figsize=(8, 5), xlim=[0, epochs], ylim=[0, 1], grid=True, xlabel=\"Epoch\",\n",
        "        style=[\"r--\", \"r--.\", \"b-\", \"b-*\"] if style is None else style)\n",
        "    plt.legend(loc=\"lower left\")\n",
        "    plt.show()\n",
        "\n",
        "    # Predictions for confusion matrix and classification report\n",
        "    Y_pred = model.predict(X_test)\n",
        "    Y_pred_classes = np.argmax(Y_pred, axis=1)\n",
        "    Y_true_classes = np.argmax(Y_test, axis=1)\n",
        "\n",
        "    # Classification report\n",
        "    print(classification_report(Y_true_classes, Y_pred_classes, target_names=class_names))\n",
        "\n",
        "    # Confusion matrix\n",
        "    cm = confusion_matrix(Y_true_classes, Y_pred_classes)\n",
        "    fig = plot_confusion_matrix(cm, class_names)\n",
        "    plt.show()\n",
        "\n",
        "def label2index(i):\n",
        "    m = {'N':0, 'S':1, 'V':2, 'F':3, 'Q':4}\n",
        "    return m[i]\n",
        "\n",
        "def load_and_preprocess_data(path, num_classes):\n",
        "    data = np.load(os.path.join(path, 'mitdb_data.npy'))\n",
        "    label_str = np.load(os.path.join(path, 'mitdb_group.npy'))\n",
        "    label = np.array([label2index(i) for i in label_str])\n",
        "\n",
        "    train_ind = np.load(os.path.join(path, 'mitdb_train_ind.npy'))\n",
        "    test_ind = np.load(os.path.join(path, 'mitdb_test_ind.npy'))\n",
        "#    data = preprocessing.scale(data, axis=1)\n",
        "    X_train = data[train_ind]\n",
        "    X_test = data[test_ind]\n",
        "    Y_train = utils.to_categorical(label[train_ind], num_classes=num_classes)\n",
        "    Y_test = utils.to_categorical(label[test_ind], num_classes=num_classes)\n",
        "\n",
        "    smote = SMOTE(random_state=30)\n",
        "    X_train, Y_train = smote.fit_resample(X_train, Y_train)\n",
        "\n",
        "    return X_train, X_test, Y_train, Y_test"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f7aa347",
      "metadata": {},
      "source": [
        "### crtnet_models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "841ca1af",
      "metadata": {
        "gather": {
          "logged": 1715408990312
        }
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from importlib import reload\n",
        "from src import crtnet_models\n",
        "reload(crtnet_models)\n",
        "\n",
        "def create_crtnet_original(number_of_leads=1, num_classes=5, multilabel=False, learning_rate=0.001):\n",
        "    tf.keras.backend.clear_session()\n",
        "    return crtnet_models.crt_net_original(\n",
        "        n_classes=num_classes,\n",
        "        input_shape=(None,number_of_leads),\n",
        "        n_vgg_blocks=1,\n",
        "        binary=multilabel, # set this to true if using multilabel output (disables softmax and categorical cross entropy).\n",
        "        use_focal=True, # addresses significant class imbalance (enables focal cross entropy)\n",
        "        metrics=['accuracy'], # May be better to evaluate on F1 score if using early stopping\n",
        "        d_model=128, # default feature dim size (d_ffn set to 2*d_model)\n",
        "        learning_rate=learning_rate\n",
        "    )\n",
        "\n",
        "def create_crtnet_alternate(number_of_leads=1, num_classes=5, multilabel=False, learning_rate=0.001):\n",
        "    tf.keras.backend.clear_session()\n",
        "    return crtnet_models.crt_net_original_alt(\n",
        "        n_classes=num_classes,\n",
        "        input_shape=(None,number_of_leads),\n",
        "        n_vgg_blocks=1,\n",
        "        binary=multilabel, # set this to true if using multilabel output (disables softmax and categorical cross entropy).\n",
        "        use_focal=True, # addresses significant class imbalance (enables focal cross entropy)\n",
        "        metrics=['accuracy'], # May be better to evaluate on F1 score if using early stopping\n",
        "        d_model=128, # default feature dim size (d_ffn set to 2*d_model)\n",
        "        learning_rate=learning_rate\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c15da661",
      "metadata": {
        "gather": {
          "logged": 1715410888774
        }
      },
      "outputs": [],
      "source": [
        "import tensorflow.keras as keras\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import datetime\n",
        "import numpy as np\n",
        "\n",
        "path = 'data/'\n",
        "num_classes = 5\n",
        "class_names = ['N', 'S', 'V', 'F', 'Q']  # Update based on your classes\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = load_and_preprocess_data(path, num_classes)\n",
        "\n",
        "use_alternate = True # if True: leaky ReLU (alpha=0.3); dropout (rate=0.2); max pos encoding = 2048; SeLU activation on pre-output\n",
        "initial_learning_rate = 0.0001\n",
        "\n",
        "create_crtnet_method = create_crtnet_alternate if use_alternate else create_crtnet_original\n",
        "model = create_crtnet_method(\n",
        "    number_of_leads=1,\n",
        "    num_classes=num_classes,\n",
        "    multilabel=False,\n",
        "    learning_rate=initial_learning_rate)\n",
        "# model already compiled. it can still be recompiled if needed\n",
        "model.summary()\n",
        "\n",
        "# Define callbacks\n",
        "stopping = keras.callbacks.EarlyStopping(patience=5)\n",
        "reduce_lr = keras.callbacks.ReduceLROnPlateau(factor=0.1, patience=3, min_lr=0.001*0.001)\n",
        "\n",
        "train_and_evaluate_model(\n",
        "    model,\n",
        "    X_train,\n",
        "    X_test,\n",
        "    Y_train,\n",
        "    Y_test,\n",
        "    class_names=class_names,\n",
        "    callbacks=[reduce_lr, stopping],\n",
        "    epochs=20,\n",
        "    batch_size=64,\n",
        "    style=[\"r--\", \"r--.\", \"b-\", \"b-*\", \"g-\", \"g-*\"] # has f1 score so add green line for this\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "tf215gpu"
    },
    "kernelspec": {
      "display_name": "tf215gpu",
      "language": "python",
      "name": "tf215gpu"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
