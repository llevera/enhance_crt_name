{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Howto\n",
        "\n",
        "\n",
        "1. Decide if wanting to process the data and evaluate the same as CRT-Net paper, ie allow whatever label classes are in the data, but for each sample, just pick the first diagnosis OR do it PhysioNet competition style where only samples from a certain number of labels (as per csv file) are used but also each sample can have multiple labels\n",
        "1. If you want CRT-Net style, run the cell where data is loaded with load_data given the argument adjust_classes_for_physionet as false. This will produce three file, the data samples themselves, the one hot encoded labels (only one per sample) and the discovered class names, to make it easier later to translate on hot labels to readable class names\n",
        "1. If you want PhysioNet style, run cell with load_data given the argument adjust_classes_for_physionet as true\n",
        "1. Run the cell that creates the train/evaluate functions plus creates the baseline crt_net model\n",
        "1. Run other cells to create other models\n",
        "1. Change the cell to read whatever numpy datafiles you like, and train/evaluate. It will work out whether to do multilabel or single label based on the onehot encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "\n",
        "The below reads .hea and .mat file pairs and then :\n",
        "\n",
        "- Standardises the sample lengths to the most common length (unless overridden), and sampling frequency to the most common frequency.\n",
        "- One hot encodes the diagnostic class label, either picking the first \"primary\" label, or encoding all labels. The first case is consistency with the CRT-Net paper, the second for consistency with PhysioNet challenge\n",
        "- To also be consistent with the PhysioNet challenge, it is also catered to restrict the classes to a certain specified set (via csv file), and also to ensure the \"normal\" class is always present\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1715769831228
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import wfdb\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "\n",
        "china_to_snomed = {\n",
        "    '1':'426783006',\n",
        "    '2':'164889003',\n",
        "    '3':'270492004',\n",
        "    '4':'164909002',\n",
        "    '5':'59118001',\n",
        "    '6':'284470004',\n",
        "    '7':'164884008',\n",
        "    '8':'429622005',\n",
        "    '9':'164931005',\n",
        "}\n",
        "\n",
        "def read_records(record_files, csv_file_path=None):\n",
        "    records = []\n",
        "    labels = []\n",
        "\n",
        "    if (csv_file_path is not None):\n",
        "        df = pd.read_csv(csv_file_path)\n",
        "\n",
        "    for record_file in record_files:\n",
        "        record = wfdb.rdrecord(record_file)\n",
        "\n",
        "        if(csv_file_path is not None):\n",
        "            recording_id = record.file_name[0].split('.')[0]  # Assuming the recording ID is in the file name\n",
        "        \n",
        "            # Extract the First_label for the corresponding recording (numbner) and map it longer number\n",
        "            first_label_str = df[df['Recording'] == recording_id]['First_label'].astype(str).values[0]\n",
        "            first_label_mapped = china_to_snomed[first_label_str]\n",
        "            \n",
        "            # append a list with the first label\n",
        "            labels.append([first_label_mapped])\n",
        "        else:\n",
        "            for comment in record.comments:\n",
        "                if comment.startswith('Dx') or comment.startswith(' Dx'):\n",
        "                    dxs = set(arr.strip() for arr in comment.split(': ')[1].split(','))\n",
        "                    labels.append(dxs)\n",
        "                    \n",
        "        records.append(wfdb.rdrecord(record_file))\n",
        "    return records, labels\n",
        "\n",
        "\n",
        "def create_one_hot_labels(all_labels, target_classes, num_recordings):\n",
        "    discard_index = list()\n",
        "    labels = np.zeros((num_recordings, len(target_classes)))  # , dtype=np.bool)\n",
        "    for i in range(num_recordings):\n",
        "        dxs = all_labels[i]\n",
        "        flag = np.zeros((1, len(dxs)), dtype=bool)\n",
        "        count = 0\n",
        "        for dx in dxs:\n",
        "            if dx in target_classes:\n",
        "                j = target_classes.index(dx)\n",
        "                labels[i, j] = 1\n",
        "                flag[0, count] = True\n",
        "\n",
        "            count += 1\n",
        "\n",
        "        # note any recordings that don't have any of the classes we are looking for\n",
        "        if np.any(flag) == False:\n",
        "            discard_index.append(i)\n",
        "\n",
        "    return labels, discard_index\n",
        "\n",
        "def get_unique_classes(all_labels, valid_classes=None):\n",
        "\n",
        "    classes2 = list()\n",
        "    for i in range(len(all_labels)):\n",
        "        dxs = all_labels[i]\n",
        "        for dx in dxs:\n",
        "            if valid_classes is None or dx in valid_classes:\n",
        "                classes2.append(dx)\n",
        "\n",
        "    classes3 = list()\n",
        "    for x in classes2:\n",
        "        if x not in classes3:\n",
        "            classes3.append(x)\n",
        "\n",
        "    classes3 = sorted (classes3)\n",
        "    return classes3\n",
        "\n",
        "def find_records(directory):\n",
        "    record_files = []\n",
        "    for dirpath, _, filenames in os.walk(directory):\n",
        "        for f in sorted(filenames):\n",
        "            file_path = os.path.join(dirpath, f)\n",
        "            if os.path.isfile(file_path) and not f.lower().startswith('.'):\n",
        "                file, ext = os.path.splitext(file_path)\n",
        "                if ext.lower() == '.hea':\n",
        "                    record_files.append(file)\n",
        "    if record_files:\n",
        "        return record_files\n",
        "    else:\n",
        "        raise IOError('No record files found.')\n",
        "\n",
        "def filter(data, labels, index):\n",
        "    labels = [labels[i] for i in range(len(labels)) if i not in index]\n",
        "    data = [data[i] for i in range(len(data)) if i not in index]\n",
        "    return labels, data\n",
        "\n",
        "def consolidate_equivalent_classes(one_hot_encoded_labels, unique_classes):\n",
        "    equivalent_classes_collection = [['713427006', '59118001'], ['284470004', '63593006'], ['427172004', '17338001']]\n",
        "\n",
        "    # For each set of equivalent class, use only one class as the representative class for the set and discard the other classes in the set.\n",
        "    # The label for the representative class is positive if any of the labels in the set is positive.\n",
        "    remove_classes = list()\n",
        "    remove_indices = list()\n",
        "    for equivalent_classes in equivalent_classes_collection:\n",
        "        equivalent_classes = [x for x in equivalent_classes if x in unique_classes]\n",
        "        if len(equivalent_classes)>1:\n",
        "            other_classes = equivalent_classes[1:]\n",
        "            equivalent_indices = [unique_classes.index(x) for x in equivalent_classes]\n",
        "            representative_index = equivalent_indices[0]\n",
        "            other_indices = equivalent_indices[1:]\n",
        "\n",
        "            one_hot_encoded_labels[:, representative_index] = np.any(one_hot_encoded_labels[:, equivalent_indices], axis=1)\n",
        "            remove_classes += other_classes\n",
        "            remove_indices += other_indices\n",
        "\n",
        "    for x in remove_classes:\n",
        "        unique_classes.remove(x)\n",
        "    one_hot_encoded_labels = np.delete(one_hot_encoded_labels, remove_indices, axis=1)\n",
        "\n",
        "    return one_hot_encoded_labels, unique_classes\n",
        "\n",
        "def set_labels_to_normal_if_none_other(labels, unique_classes, normal_class):\n",
        "    # If the labels are negative for all classes, then change the label for the normal class to positive.\n",
        "    normal_index = unique_classes.index(normal_class)\n",
        "    for i in range(len(labels)):\n",
        "        num_positive_classes = np.sum(labels[i, :])\n",
        "        if num_positive_classes==0:\n",
        "            labels[i, normal_index] = 1\n",
        "\n",
        "    return labels\n",
        "\n",
        "def ensure_normal_class(unique_classes, normal_class):\n",
        "    if normal_class not in unique_classes:\n",
        "        unique_classes.add(normal_class)\n",
        "        print('- The normal class {} is not one of the label classes, so it has been automatically added, but please check that you chose the correct normal class.'.format(normal_class))\n",
        "    unique_classes = sorted(unique_classes)\n",
        "    return unique_classes\n",
        "\n",
        "def read_scored_classes():\n",
        "    scored = list()\n",
        "    with open('dx_mapping_scored.csv', 'r') as f:\n",
        "        for l in f:\n",
        "            dxs = (l.split(','))\n",
        "            scored.append(dxs[1])\n",
        "    return (sorted(scored[1:]))\n",
        "\n",
        "def filter_out(one_hot_encoded_labels, records, discard_index):\n",
        "    one_hot_encoded_labels = [one_hot_encoded_labels[i] for i in range(len(one_hot_encoded_labels)) if i not in discard_index]\n",
        "    records = [records[i] for i in range(len(records)) if i not in discard_index]\n",
        "\n",
        "    return one_hot_encoded_labels, records\n",
        "\n",
        "def load_records(record_file_list, adjust_classes_for_physionet, csv_file, normal_class):\n",
        "        \n",
        "    if len(record_file_list) == 0:\n",
        "        raise ValueError('No record files found.')\n",
        "\n",
        "    num_recordings = len(record_file_list)\n",
        "\n",
        "    records, all_labels = read_records(record_file_list, csv_file)\n",
        "\n",
        "    scored = None\n",
        "    if adjust_classes_for_physionet:\n",
        "        scored = read_scored_classes()\n",
        "\n",
        "    unique_classes = get_unique_classes(all_labels, scored)\n",
        "\n",
        "    if (normal_class is not None):\n",
        "        unique_classes = ensure_normal_class(unique_classes, normal_class)\n",
        "    \n",
        "    one_hot_encoded_labels, discard_index = create_one_hot_labels(all_labels, unique_classes, num_recordings)\n",
        "\n",
        "    if (adjust_classes_for_physionet):\n",
        "        one_hot_encoded_labels, unique_classes = consolidate_equivalent_classes(one_hot_encoded_labels, unique_classes)\n",
        "    \n",
        "    if (normal_class is not None):\n",
        "        one_hot_encoded_labels = set_labels_to_normal_if_none_other(one_hot_encoded_labels, unique_classes, normal_class)\n",
        "    \n",
        "    one_hot_encoded_labels, records = filter_out(one_hot_encoded_labels, records, discard_index)\n",
        "\n",
        "    return one_hot_encoded_labels, records, unique_classes\n",
        "\n",
        "def standardise_length(data, target_length):    \n",
        "    if len(data)>=target_length:\n",
        "        return data[:target_length, :]\n",
        "    \n",
        "    return np.pad(data, ((0, target_length - data.shape[0]), (0, 0)), mode='constant')\n",
        "\n",
        "def resample(data, src_frq, trg_frq):\n",
        "    if src_frq == trg_frq:\n",
        "        return data\n",
        "\n",
        "    N_src = data.shape[1]\n",
        "    N_trg = int(N_src * trg_frq / src_frq)\n",
        "    \n",
        "    resampled = np.zeros((data.shape[0], N_trg), dtype='float32')\n",
        "    for i in range(data.shape[0]):\n",
        "        resampled[i] = np.interp(np.linspace(0, N_src, N_trg), np.arange(N_src), data[i])\n",
        "        \n",
        "    return resampled\n",
        "\n",
        "def standardise_data_samples(records, fixed_sample_length=None):\n",
        "    standardised_data = list()\n",
        "\n",
        "    # find the most common fs\n",
        "    fss = [record.fs for record in records]\n",
        "    target_fs = max(set(fss), key=fss.count)\n",
        "\n",
        "    # find the most common sig_len\n",
        "    sig_lens = [record.sig_len for record in records]\n",
        "    if fixed_sample_length is not None:\n",
        "        target_length = fixed_sample_length\n",
        "    else:\n",
        "        target_length = max(set(sig_lens), key=sig_lens.count)\n",
        "\n",
        "    for i in range(len(records)):\n",
        "        datum = records[i].p_signal\n",
        "        datum = resample(datum, records[i].fs, target_fs)\n",
        "        datum = standardise_length(datum, target_length)\n",
        "        standardised_data.append(datum)\n",
        "\n",
        "    return standardised_data\n",
        "\n",
        "def load_data(input_directory, adjust_classes_for_physionet=False, csv_file=None, normal_class=None, fixed_sample_length=None):\n",
        "    record_file_list = find_records(input_directory)\n",
        "    \n",
        "    one_hot_encoded_labels, records, classes = load_records(record_file_list, adjust_classes_for_physionet, csv_file=csv_file, normal_class=normal_class)\n",
        "    samples = standardise_data_samples(records, fixed_sample_length=fixed_sample_length)\n",
        "            \n",
        "    one_hot_encoded_labels = np.stack(one_hot_encoded_labels, axis =0)\n",
        "    samples = np.stack(samples, axis =0)\n",
        "\n",
        "    return one_hot_encoded_labels, samples, classes\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Run below to get an numpy output of samples, single one hot encoded label and the class names for the China 2018 data as used by the CRT-Net paper."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1715770352783
        }
      },
      "outputs": [],
      "source": [
        "output_directory = 'data'\n",
        "input_directory = 'cpsc_2018'\n",
        "    \n",
        "one_hot_encoding_labels, samples, classes = load_data(input_directory, adjust_classes_for_physionet=False,  csv_file='REFERENCE.csv', fixed_sample_length=3000)\n",
        "\n",
        "# maps SNOMED CT codes to classes\n",
        "snomed_to_abbreviation = {\n",
        "    '164884008': 'PVC',\n",
        "    '164889003': 'AF',\n",
        "    '164909002': 'LBBB',\n",
        "    '164931005': 'STE',\n",
        "    '270492004': 'IAVB',\n",
        "    '284470004': 'PAC',\n",
        "    '426783006': 'N',\n",
        "    '429622005': 'STD',\n",
        "    '59118001': 'RBBB'\n",
        "}\n",
        "\n",
        "# map classes to abbreviations\n",
        "mapped_abbreviations = [snomed_to_abbreviation.get(code, None) for code in classes]\n",
        "\n",
        "# save the data to a file\n",
        "np.save(os.path.join(output_directory, 'cpsc_2018_crtnet_singlelabel_samples3.npy'), samples)\n",
        "np.save(os.path.join(output_directory, 'cpsc_2018_crtnet_singlelabel_one_hot_encoding_labels3.npy'), one_hot_encoding_labels)\n",
        "np.save(os.path.join(output_directory, 'cpsc_2018_crtnet_singlelabel_classes3.npy'), mapped_abbreviations)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1715708491364
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "output_directory = 'data'\n",
        "input_directory = 'cpsc_2018'\n",
        "    \n",
        "one_hot_encoding_labels, samples, classes = load_data(input_directory, adjust_classes_for_physionet=False)\n",
        "\n",
        "# maps SNOMED CT codes to classes\n",
        "snomed_to_abbreviation = {\n",
        "    '164884008': 'PVC',\n",
        "    '164889003': 'AF',\n",
        "    '164909002': 'LBBB',\n",
        "    '164931005': 'STE',\n",
        "    '270492004': 'IAVB',\n",
        "    '284470004': 'PAC',\n",
        "    '426783006': 'N',\n",
        "    '429622005': 'STD',\n",
        "    '59118001': 'RBBB'\n",
        "}\n",
        "\n",
        "# map classes to abbreviations\n",
        "mapped_abbreviations = [snomed_to_abbreviation.get(code, None) for code in classes]\n",
        "\n",
        "# save the data to a file\n",
        "np.save(os.path.join(output_directory, 'cpsc_2018_crtnet_multilabel_samples.npy'), samples)\n",
        "np.save(os.path.join(output_directory, 'cpsc_2018_crtnet_multilabel_one_hot_encoding_labels.npy'), one_hot_encoding_labels)\n",
        "np.save(os.path.join(output_directory, 'cpsc_2018_crtnet_multilabel_classes.npy'), mapped_abbreviations)\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import wfdb\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# get stats of the data\n",
        "samples = np.load(os.path.join(\"data\", 'cpsc_2018_crtnet_multilabel_samples.npy'))\n",
        "one_hot_encoding_labels = np.load(os.path.join(\"data\", 'cpsc_2018_crtnet_multilabel_one_hot_encoding_labels.npy'))\n",
        "classes = np.load(os.path.join(\"data\", 'cpsc_2018_crtnet_multilabel_classes.npy'))\n",
        "\n",
        "print(f\"Number of samples: {samples.shape[0]}\")\n",
        "print(f\"Number of leads: {samples.shape[2]}\")\n",
        "print(f\"Number of classes: {one_hot_encoding_labels.shape[1]}\")\n",
        "print(f\"Classes: {classes}\")\n",
        "\n",
        "# get counts of each class\n",
        "class_counts = np.sum(one_hot_encoding_labels, axis=0)\n",
        "class_counts = dict(zip(classes, class_counts))\n",
        "print(f\"Class counts: {class_counts}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Run below to get an numpy output of samples, multilabel one hot encoded labels and the class names for any PhysioNet data. This data will be restricted to the PhysioNet challenge list of classes and will be guaranteed to include the \"normal\" class.\n",
        "\n",
        "The can be run on any set of sets of PhysioNet data, including China 2018, PTB-XL etc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1715304125386
        }
      },
      "outputs": [],
      "source": [
        "output_directory = 'data'\n",
        "input_directory = 'cpsc_2018'\n",
        "    \n",
        "one_hot_encoding_labels, samples, classes = load_data(input_directory, adjust_classes_for_physionet=True, normal_class='426783006')\n",
        "# save the data to a file\n",
        "np.save(os.path.join(output_directory, 'cpsc_2018_physionet_samples.npy'), samples)\n",
        "np.save(os.path.join(output_directory, 'cpsc_2018_physionet_one_hot_encoding_labels.npy'), one_hot_encoding_labels)\n",
        "np.save(os.path.join(output_directory, 'cpsc_2018_physionet_classes.npy'), classes)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Load data from numpy files, then train and evaluate of whichever model, by default the baseline CRT-Net\n",
        "\n",
        "The loss function depends on whether doing multilabel or single label classification, which is inferred from the data (ie if each sample has exlusive one hot encoding or multiple)\n",
        "\n",
        "The class names are read to label the outputs more clearly based on SNOMED, vs just having unlabeled one-hot encoding position"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1715710232706
        }
      },
      "outputs": [],
      "source": [
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras import utils\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import datetime\n",
        "import numpy as np\n",
        "from src import train_and_evaluate\n",
        "from importlib import reload\n",
        "reload(train_and_evaluate)\n",
        "from src import crtnet_models\n",
        "reload(crtnet_models)\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "\n",
        "    \n",
        "stopping = keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True)\n",
        "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, min_lr=0.00001)\n",
        "\n",
        "# load the data from the file\n",
        "samples = np.load(os.path.join(\"data\", 'cpsc_2018_crtnet_multilabel_samples.npy'))\n",
        "one_hot_encoding_labels = np.load(os.path.join(\"data\", 'cpsc_2018_crtnet_multilabel_one_hot_encoding_labels.npy'))\n",
        "classes = np.load(os.path.join(\"data\", 'cpsc_2018_crtnet_multilabel_classes.npy'))\n",
        "\n",
        "is_multilabel = any(sum(row) > 1 for row in one_hot_encoding_labels)\n",
        "\n",
        "create_crtnet_method = crtnet_models.create_crtnet_alternate\n",
        "\n",
        "train_and_evaluate.train_and_evaluate_model(\n",
        "    create_crtnet_method,\n",
        "    samples=samples,\n",
        "    one_hot_encoding_labels=one_hot_encoding_labels,\n",
        "    callbacks=[reduce_lr, stopping],\n",
        "    epochs=2,\n",
        "    batch_size=128,\n",
        "    classes=classes,\n",
        "    folds=2,\n",
        "    number_of_leads=samples.shape[2],\n",
        "    is_multilabel=is_multilabel,\n",
        "    initial_learning_rate=0.0001\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "gather": {
          "logged": 1715760031269
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras import utils\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from src import train_and_evaluate\n",
        "from importlib import reload\n",
        "reload(train_and_evaluate)\n",
        "from src import crtnet_models\n",
        "reload(crtnet_models)\n",
        "import os\n",
        "import datetime\n",
        "import pickle\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "\n",
        "EPOCHS = 50\n",
        "BATCH_SIZE = 128\n",
        "INITIAL_LEARNING_RATE = 0.0001\n",
        "OUTPUT_DIRECTORY = \"output\"\n",
        "\n",
        "# Callback Constants\n",
        "EARLY_STOPPING_MONITOR = 'val_accuracy'\n",
        "EARLY_STOPPING_PATIENCE = 5\n",
        "EARLY_STOPPING_RESTORE_BEST_WEIGHTS = True\n",
        "\n",
        "REDUCE_LR_MONITOR = 'val_loss'\n",
        "REDUCE_LR_FACTOR = 0.5\n",
        "REDUCE_LR_PATIENCE = 4\n",
        "REDUCE_LR_MIN_LR = 0.00001\n",
        "\n",
        "# Define folds\n",
        "FOLDS = None\n",
        "\n",
        "def save_run_data(run_data, directory=OUTPUT_DIRECTORY, filename=None):\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "    \n",
        "    if filename is None:\n",
        "        timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        filename = f\"run_data_{timestamp}.pkl\"\n",
        "    \n",
        "    filepath = os.path.join(directory, filename)\n",
        "    \n",
        "    with open(filepath, 'wb') as f:\n",
        "        pickle.dump(run_data, f)\n",
        "    \n",
        "    return filepath\n",
        "\n",
        "def load_run_data(filepath):\n",
        "    with open(filepath, 'rb') as f:\n",
        "        return pickle.load(f)\n",
        "\n",
        "# Define the model creation methods\n",
        "model_methods = [\n",
        "    crtnet_models.create_crtnet_alternate,\n",
        "    #crtnet_models.create_crtnet_our_transformer,\n",
        "    #crtnet_models.create_crtnet_dense,\n",
        "    #crtnet_models.create_crtnet_no_attn,\n",
        "    #crtnet_models.create_crtnet_dense_noselu,\n",
        "#    crtnet_models.create_crtnet_alternate,\n",
        "    #crtnet_models.create_crtnet_rwkv,\n",
        "]\n",
        "\n",
        "# Load data once\n",
        "multilabel_data = {\n",
        "    \"samples\": np.load(os.path.join(\"data\", 'cpsc_2018_crtnet_multilabel_samples.npy')),\n",
        "    \"one_hot_encoding_labels\": np.load(os.path.join(\"data\", 'cpsc_2018_crtnet_multilabel_one_hot_encoding_labels.npy')),\n",
        "    \"classes\": np.load(os.path.join(\"data\", 'cpsc_2018_crtnet_multilabel_classes.npy'))\n",
        "}\n",
        "\n",
        "singlelabel_data = {\n",
        "    \"samples\": np.load(os.path.join(\"data\", 'cpsc_2018_crtnet_singlelabel_samples.npy')),\n",
        "    \"one_hot_encoding_labels\": np.load(os.path.join(\"data\", 'cpsc_2018_crtnet_singlelabel_one_hot_encoding_labels.npy')),\n",
        "    \"classes\": np.load(os.path.join(\"data\", 'cpsc_2018_crtnet_singlelabel_classes.npy'))\n",
        "}\n",
        "\n",
        "# Function to determine if labels are multilabel\n",
        "def is_multilabel(labels):\n",
        "    return any(sum(row) > 1 for row in labels)\n",
        "\n",
        "data = singlelabel_data\n",
        "all_runs_data = []\n",
        "\n",
        "# Loop through model methods, data types, and folds\n",
        "for create_crtnet_method in model_methods:\n",
        "    run_results = train_and_evaluate.train_and_evaluate_model(\n",
        "        create_crtnet_method,\n",
        "        samples=data[\"samples\"],\n",
        "        one_hot_encoding_labels=data[\"one_hot_encoding_labels\"],\n",
        "        callbacks=[\n",
        "            keras.callbacks.EarlyStopping(monitor=EARLY_STOPPING_MONITOR, patience=EARLY_STOPPING_PATIENCE, restore_best_weights=EARLY_STOPPING_RESTORE_BEST_WEIGHTS),\n",
        "            keras.callbacks.ReduceLROnPlateau(monitor=REDUCE_LR_MONITOR, factor=REDUCE_LR_FACTOR, patience=REDUCE_LR_PATIENCE, min_lr=REDUCE_LR_MIN_LR)\n",
        "        ],\n",
        "        epochs=EPOCHS,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        classes=data[\"classes\"],\n",
        "        folds=FOLDS,\n",
        "        number_of_leads=data[\"samples\"].shape[2],\n",
        "        is_multilabel=is_multilabel(data[\"one_hot_encoding_labels\"]),\n",
        "        initial_learning_rate=INITIAL_LEARNING_RATE\n",
        "    )\n",
        "\n",
        "    run_data = {\n",
        "        'create_crtnet_method': create_crtnet_method.__name__,\n",
        "        'epochs': EPOCHS,\n",
        "        'batch_size': BATCH_SIZE,\n",
        "        'initial_learning_rate': INITIAL_LEARNING_RATE,\n",
        "        'early_stopping': {\n",
        "            'monitor': EARLY_STOPPING_MONITOR,\n",
        "            'patience': EARLY_STOPPING_PATIENCE,\n",
        "            'restore_best_weights': EARLY_STOPPING_RESTORE_BEST_WEIGHTS\n",
        "        },\n",
        "        'reduce_lr': {\n",
        "            'monitor': REDUCE_LR_MONITOR,\n",
        "            'factor': REDUCE_LR_FACTOR,\n",
        "            'patience': REDUCE_LR_PATIENCE,\n",
        "            'min_lr': REDUCE_LR_MIN_LR\n",
        "        },\n",
        "        'folds_data': run_results['folds_data'],\n",
        "        'training_run_summary': run_results['training_run_summary']\n",
        "    }\n",
        "\n",
        "    all_runs_data.append(run_data)\n",
        "\n",
        "save_run_data(all_runs_data)\n"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "tf215gpu"
    },
    "kernelspec": {
      "display_name": "tf215gpu",
      "language": "python",
      "name": "tf215gpu"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
