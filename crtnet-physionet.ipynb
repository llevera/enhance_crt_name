{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Howto\n",
        "\n",
        "\n",
        "1. Decide if wanting to process the data and evaluate the same as CRT-Net paper, ie allow whatever label classes are in the data, but for each sample, just pick the first diagnosis OR do it PhysioNet competition style where only samples from a certain number of labels (as per csv file) are used but also each sample can have multiple labels\n",
        "1. If you want CRT-Net style, run the cell where data is loaded with load_data given the argument adjust_classes_for_physionet as false. This will produce three file, the data samples themselves, the one hot encoded labels (only one per sample) and the discovered class names, to make it easier later to translate on hot labels to readable class names\n",
        "1. If you want PhysioNet style, run cell with load_data given the argument adjust_classes_for_physionet as true\n",
        "1. Run the cell that creates the train/evaluate functions plus creates the baseline crt_net model\n",
        "1. Run other cells to create other models\n",
        "1. Change the cell to read whatever numpy datafiles you like, and train/evaluate. It will work out whether to do multilabel or single label based on the onehot encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "\n",
        "The below reads .hea and .mat file pairs and then :\n",
        "\n",
        "- Standardises the sample lengths to the most common length (unless overridden), and sampling frequency to the most common frequency.\n",
        "- One hot encodes the diagnostic class label, either picking the first \"primary\" label, or encoding all labels. The first case is consistency with the CRT-Net paper, the second for consistency with PhysioNet challenge\n",
        "- To also be consistent with the PhysioNet challenge, it is also catered to restrict the classes to a certain specified set (via csv file), and also to ensure the \"normal\" class is always present\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "gather": {
          "logged": 1715303075414
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import wfdb\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def read_records(record_files):\n",
        "    records = []\n",
        "    labels = []\n",
        "    for record_file in record_files:\n",
        "        record = wfdb.rdrecord(record_file)\n",
        "        if record.file_name[0].endswith('.dat'):\n",
        "            # TODO work out how to deal with MIT-BIH with its different hea/atr/dat files\n",
        "            # and very low samples. Split into many files?\n",
        "            ann = wfdb.rdann(record_file,'atr')\n",
        "        else:\n",
        "            for comment in record.comments:\n",
        "                if comment.startswith('Dx') or comment.startswith(' Dx'):\n",
        "                    dxs = set(arr.strip() for arr in comment.split(': ')[1].split(','))\n",
        "                    labels.append(dxs)\n",
        "                    \n",
        "        records.append(wfdb.rdrecord(record_file))\n",
        "    return records, labels\n",
        "\n",
        "\n",
        "def create_one_hot_labels(all_labels, target_classes, num_recordings, adjust_classes_for_physionet):\n",
        "    discard_index = list()\n",
        "    labels = np.zeros((num_recordings, len(target_classes)))  # , dtype=np.bool)\n",
        "    for i in range(num_recordings):\n",
        "        dxs = all_labels[i]\n",
        "        flag = np.zeros((1, len(dxs)), dtype=bool)\n",
        "        count = 0\n",
        "        for dx in dxs:\n",
        "            if dx in target_classes:\n",
        "                j = target_classes.index(dx)\n",
        "                labels[i, j] = 1\n",
        "                flag[0, count] = True\n",
        "                \n",
        "                # Break out of the loop if adjust_classes_for_physionet is not set\n",
        "                if not adjust_classes_for_physionet:\n",
        "                    break\n",
        "\n",
        "            count += 1\n",
        "\n",
        "        # note any recordings that don't have any of the classes we are looking for\n",
        "        if np.any(flag) == False:\n",
        "            discard_index.append(i)\n",
        "\n",
        "    return labels, discard_index\n",
        "\n",
        "def get_unique_classes(all_labels, valid_classes=None):\n",
        "\n",
        "    classes2 = list()\n",
        "    for i in range(len(all_labels)):\n",
        "        dxs = all_labels[i]\n",
        "        for dx in dxs:\n",
        "            if valid_classes is None or dx in valid_classes:\n",
        "                classes2.append(dx)\n",
        "\n",
        "    classes3 = list()\n",
        "    for x in classes2:\n",
        "        if x not in classes3:\n",
        "            classes3.append(x)\n",
        "\n",
        "    classes3 = sorted (classes3)\n",
        "    return classes3\n",
        "\n",
        "def find_records(directory):\n",
        "    record_files = []\n",
        "    for dirpath, _, filenames in os.walk(directory):\n",
        "        for f in sorted(filenames):\n",
        "            file_path = os.path.join(dirpath, f)\n",
        "            if os.path.isfile(file_path) and not f.lower().startswith('.'):\n",
        "                file, ext = os.path.splitext(file_path)\n",
        "                if ext.lower() == '.hea':\n",
        "                    record_files.append(file)\n",
        "    if record_files:\n",
        "        return record_files\n",
        "    else:\n",
        "        raise IOError('No record files found.')\n",
        "\n",
        "def filter(data, labels, index):\n",
        "    labels = [labels[i] for i in range(len(labels)) if i not in index]\n",
        "    data = [data[i] for i in range(len(data)) if i not in index]\n",
        "    return labels, data\n",
        "\n",
        "def consolidate_equivalent_classes(one_hot_encoded_labels, unique_classes):\n",
        "    equivalent_classes_collection = [['713427006', '59118001'], ['284470004', '63593006'], ['427172004', '17338001']]\n",
        "\n",
        "    # For each set of equivalent class, use only one class as the representative class for the set and discard the other classes in the set.\n",
        "    # The label for the representative class is positive if any of the labels in the set is positive.\n",
        "    remove_classes = list()\n",
        "    remove_indices = list()\n",
        "    for equivalent_classes in equivalent_classes_collection:\n",
        "        equivalent_classes = [x for x in equivalent_classes if x in unique_classes]\n",
        "        if len(equivalent_classes)>1:\n",
        "            other_classes = equivalent_classes[1:]\n",
        "            equivalent_indices = [unique_classes.index(x) for x in equivalent_classes]\n",
        "            representative_index = equivalent_indices[0]\n",
        "            other_indices = equivalent_indices[1:]\n",
        "\n",
        "            one_hot_encoded_labels[:, representative_index] = np.any(one_hot_encoded_labels[:, equivalent_indices], axis=1)\n",
        "            remove_classes += other_classes\n",
        "            remove_indices += other_indices\n",
        "\n",
        "    for x in remove_classes:\n",
        "        unique_classes.remove(x)\n",
        "    one_hot_encoded_labels = np.delete(one_hot_encoded_labels, remove_indices, axis=1)\n",
        "\n",
        "    return one_hot_encoded_labels, unique_classes\n",
        "\n",
        "def set_labels_to_normal_if_none_other(labels, unique_classes, normal_class):\n",
        "    # If the labels are negative for all classes, then change the label for the normal class to positive.\n",
        "    normal_index = unique_classes.index(normal_class)\n",
        "    for i in range(len(labels)):\n",
        "        num_positive_classes = np.sum(labels[i, :])\n",
        "        if num_positive_classes==0:\n",
        "            labels[i, normal_index] = 1\n",
        "\n",
        "    return labels\n",
        "\n",
        "def ensure_normal_class(unique_classes, normal_class):\n",
        "    if normal_class not in unique_classes:\n",
        "        unique_classes.add(normal_class)\n",
        "        print('- The normal class {} is not one of the label classes, so it has been automatically added, but please check that you chose the correct normal class.'.format(normal_class))\n",
        "    unique_classes = sorted(unique_classes)\n",
        "    return unique_classes\n",
        "\n",
        "def read_scored_classes():\n",
        "    scored = list()\n",
        "    with open('dx_mapping_scored.csv', 'r') as f:\n",
        "        for l in f:\n",
        "            dxs = (l.split(','))\n",
        "            scored.append(dxs[1])\n",
        "    return (sorted(scored[1:]))\n",
        "\n",
        "def filter_out(one_hot_encoded_labels, records, discard_index):\n",
        "    one_hot_encoded_labels = [one_hot_encoded_labels[i] for i in range(len(one_hot_encoded_labels)) if i not in discard_index]\n",
        "    records = [records[i] for i in range(len(records)) if i not in discard_index]\n",
        "\n",
        "    return one_hot_encoded_labels, records\n",
        "\n",
        "def load_records(record_file_list, adjust_classes_for_physionet, normal_class):\n",
        "        \n",
        "    if len(record_file_list) == 0:\n",
        "        raise ValueError('No record files found.')\n",
        "\n",
        "    num_recordings = len(record_file_list)\n",
        "\n",
        "    records, all_labels = read_records(record_file_list)\n",
        "\n",
        "    scored = None\n",
        "    if adjust_classes_for_physionet:\n",
        "        scored = read_scored_classes()\n",
        "\n",
        "    unique_classes = get_unique_classes(all_labels, scored)\n",
        "\n",
        "    if (normal_class is not None):\n",
        "        unique_classes = ensure_normal_class(unique_classes, normal_class)\n",
        "    \n",
        "    one_hot_encoded_labels, discard_index = create_one_hot_labels(all_labels, unique_classes, num_recordings, adjust_classes_for_physionet=adjust_classes_for_physionet)\n",
        "\n",
        "    if (adjust_classes_for_physionet):\n",
        "        one_hot_encoded_labels, unique_classes = consolidate_equivalent_classes(one_hot_encoded_labels, unique_classes)\n",
        "    \n",
        "    if (normal_class is not None):\n",
        "        one_hot_encoded_labels = set_labels_to_normal_if_none_other(one_hot_encoded_labels, unique_classes, normal_class)\n",
        "    \n",
        "    one_hot_encoded_labels, records = filter_out(one_hot_encoded_labels, records, discard_index)\n",
        "\n",
        "    return one_hot_encoded_labels, records, unique_classes\n",
        "\n",
        "def standardise_length(data, target_length):\n",
        "    number_of_leads = data.shape[0]\n",
        "    \n",
        "    if len(data[0])<=target_length:\n",
        "        ext= np.zeros([number_of_leads,target_length])\n",
        "        for i in range(0,number_of_leads):\n",
        "            ext[i][0:len(data[i])]=data[i]\n",
        "        return ext.T  \n",
        "    else:\n",
        "        cut = np.zeros([number_of_leads,target_length])\n",
        "        for i in range(number_of_leads):\n",
        "            tocut = len(data[0])- target_length\n",
        "            cut[i] = data[i][tocut:]\n",
        "        return cut.T \n",
        "\n",
        "def resample(data, src_frq, trg_frq):\n",
        "\n",
        "    if src_frq == trg_frq:\n",
        "        return data\n",
        "\n",
        "    N_src = data.shape[0]\n",
        "    N_trg = int(N_src * trg_frq / src_frq)\n",
        "    \n",
        "    resampled = np.zeros((N_trg, data.shape[1]), dtype='float32')\n",
        "    for i in range(data.shape[1]):\n",
        "        resampled[:,i] = np.interp(np.linspace(0, N_src, N_trg), np.arange(N_src), data[:, i])\n",
        "        \n",
        "    return resampled\n",
        "\n",
        "def standardise_data_samples(records):\n",
        "    standardised_data = list()\n",
        "\n",
        "    # find the most common fs\n",
        "    fss = [record.fs for record in records]\n",
        "    target_fs = max(set(fss), key=fss.count)\n",
        "\n",
        "    # find the most common sig_len\n",
        "    sig_lens = [record.sig_len for record in records]\n",
        "    target_length = max(set(sig_lens), key=sig_lens.count)\n",
        "\n",
        "    for i in range(len(records)):\n",
        "        datum = records[i].p_signal.T\n",
        "        datum = resample(datum,records[i].fs, target_fs)\n",
        "        datum = standardise_length(datum, target_length)\n",
        "        standardised_data.append(datum)\n",
        "\n",
        "    return standardised_data\n",
        "\n",
        "def load_data(input_directory, adjust_classes_for_physionet=False, normal_class=None):\n",
        "    record_file_list = find_records(input_directory)\n",
        "    \n",
        "    one_hot_encoded_labels, records, classes = load_records(record_file_list, adjust_classes_for_physionet, normal_class=normal_class)\n",
        "    samples = standardise_data_samples(records)\n",
        "            \n",
        "    one_hot_encoded_labels = np.stack(one_hot_encoded_labels, axis =0)\n",
        "    samples = np.stack(samples, axis =0)\n",
        "\n",
        "    return one_hot_encoded_labels, samples, classes\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Run below to get an numpy output of samples, single one hot encoded label and the class names for the China 2018 data as used by the CRT-Net paper."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "gather": {
          "logged": 1715294381581
        }
      },
      "outputs": [],
      "source": [
        "output_directory = 'data'\n",
        "input_directory = 'cpsc_2018'\n",
        "    \n",
        "one_hot_encoding_labels, samples, classes = load_data(input_directory, adjust_classes_for_physionet=False)\n",
        "\n",
        "# save the data to a file\n",
        "np.save(os.path.join(output_directory, 'cpsc_2018_crtnet_samples.npy'), samples)\n",
        "np.save(os.path.join(output_directory, 'cpsc_2018_crtnet_one_hot_encoding_labels.npy'), one_hot_encoding_labels)\n",
        "np.save(os.path.join(output_directory, 'cpsc_2018_crtnet_classes.npy'), classes)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Run below to get an numpy output of samples, multilabel one hot encoded labels and the class names for any PhysioNet data. This data will be restricted to the PhysioNet challenge list of classes and will be guaranteed to include the \"normal\" class.\n",
        "\n",
        "The can be run on any set of sets of PhysioNet data, including China 2018, PTB-XL etc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "gather": {
          "logged": 1715304125386
        }
      },
      "outputs": [],
      "source": [
        "output_directory = 'data'\n",
        "input_directory = 'cpsc_2018'\n",
        "    \n",
        "one_hot_encoding_labels, samples, classes = load_data(input_directory, adjust_classes_for_physionet=True, normal_class='426783006')\n",
        "# save the data to a file\n",
        "np.save(os.path.join(output_directory, 'cpsc_2018_physionet_samples.npy'), samples)\n",
        "np.save(os.path.join(output_directory, 'cpsc_2018_physionet_one_hot_encoding_labels.npy'), one_hot_encoding_labels)\n",
        "np.save(os.path.join(output_directory, 'cpsc_2018_physionet_classes.npy'), classes)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Create functions to \n",
        "a) Train and evaluate a model, be it multilabel or single label\n",
        "b) Create our best attempt at the matching architecture, regularisation and hyperparameters for CRT-Net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "gather": {
          "logged": 1715304860363
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "physical device not found\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras import layers\n",
        "import keras_nlp as nlp \n",
        "import tensorflow.keras as keras\n",
        "from sklearn.metrics import classification_report, multilabel_confusion_matrix\n",
        "from numpy import argmax\n",
        "import tensorflow as tf\n",
        "\n",
        "def train_and_evaluate_model(model, samples, one_hot_encoding_labels, classes, multilabel, callbacks=None, epochs=10, batch_size=64):\n",
        "    \n",
        "    train_x, validation_x, train_y, validation_y = train_test_split(samples, one_hot_encoding_labels, test_size=0.2, random_state=42)\n",
        "    history = model.fit(train_x, train_y, epochs=epochs, batch_size=batch_size, validation_data=(validation_x, validation_y), callbacks=callbacks)\n",
        "\n",
        "    pd.DataFrame(history.history).plot(\n",
        "        figsize=(8, 5), xlim=[0, epochs], ylim=[0, 1], grid=True, xlabel=\"Epoch\",\n",
        "        style=[\"r--\", \"r--.\", \"b-\", \"b-*\"])\n",
        "    plt.legend(loc=\"lower left\")\n",
        "    plt.show()\n",
        "        \n",
        "    # Classification Report\n",
        "    if classes is None:\n",
        "        classes = [\"Class \" + str(i) for i in range(len(np.unique(validation_y)))]\n",
        "\n",
        "    y_pred = model.predict(validation_x)\n",
        "\n",
        "    if (multilabel):\n",
        "        y_pred = (y_pred > 0.5).astype(int)\n",
        "\n",
        "        report = classification_report(validation_y, y_pred,  labels=classes, zero_division=0)\n",
        "        print(report)\n",
        "\n",
        "    else:\n",
        "        validation_y = np.argmax(validation_y, axis=1)\n",
        "        y_pred = np.argmax(y_pred, axis=1)\n",
        "\n",
        "        report = classification_report(y_true=validation_y, y_pred=y_pred, labels=np.unique(validation_y), target_names=classes,zero_division=0)\n",
        "        print(report)\n",
        "\n",
        "        # Confusion Matrix\n",
        "        cm = confusion_matrix(validation_y, y_pred)\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        sns.heatmap(cm, annot=True, fmt=\"d\", cmap='Reds', xticklabels=classes, yticklabels=classes)\n",
        "        plt.ylabel('True label')\n",
        "        plt.xlabel('Predicted label')\n",
        "        plt.title('Confusion Matrix')\n",
        "        plt.show()\n",
        "\n",
        "def vgg_block(input, cnn_units):\n",
        "    output = input\n",
        "    output = layers.Conv1D(cnn_units, 3, padding='same', activation='relu')(output)\n",
        "    output = layers.BatchNormalization()(output)\n",
        "    output = layers.Dropout(0.1)(output)\n",
        "    output = layers.Conv1D(cnn_units, 3, padding='same', activation='relu')(output)\n",
        "    output = layers.BatchNormalization()(output)\n",
        "    output = layers.Dropout(0.1)(output)\n",
        "    output = layers.Conv1D(cnn_units, 24, padding='same', activation='relu')(output)\n",
        "    output = layers.MaxPooling1D(2, padding='same')(output)\n",
        "    return output\n",
        "\n",
        "def create_crt_baseline(number_of_leads=None, cnn_units=128, vgg_blocks=3, rnn_units=64,\n",
        "                   transformer_encoders=4, att_dim=64, att_heads=8, fnn_units=64, num_classes=5, multilabel = False):\n",
        "    input = layers.Input(shape=(None, number_of_leads))\n",
        "    output = input\n",
        "\n",
        "    for _ in range(vgg_blocks):\n",
        "        output = layers.BatchNormalization()(output)\n",
        "        output = vgg_block(output, cnn_units)\n",
        "\n",
        "    output = layers.BatchNormalization()(output)\n",
        "    output = layers.Bidirectional(layers.GRU(rnn_units, return_sequences=True), merge_mode='sum')(output)\n",
        "    output = layers.BatchNormalization()(output)\n",
        "\n",
        "    if transformer_encoders > 0:\n",
        "        output = output + nlp.layers.SinePositionEncoding(max_wavelength=10000)(output)\n",
        "\n",
        "        for _ in range(transformer_encoders):\n",
        "            output = layers.BatchNormalization()(output)\n",
        "            output = nlp.layers.TransformerEncoder(att_dim, att_heads)(output)\n",
        "\n",
        "        output = layers.GlobalAveragePooling1D()(output)\n",
        "\n",
        "    output = layers.Dropout(0.2)(output)\n",
        "    output = layers.Dense(fnn_units, activation='relu')(output)\n",
        "    output = layers.BatchNormalization()(output)\n",
        "    output = layers.Dropout(0.2)(output)\n",
        "    output = layers.Dense(fnn_units, activation='relu')(output)\n",
        "    output = layers.BatchNormalization()(output)\n",
        "\n",
        "    if multilabel:\n",
        "        output = layers.Dense(num_classes, activation='sigmoid')(output)\n",
        "    else:\n",
        "        output = layers.Dense(num_classes, activation='softmax')(output)\n",
        "    model = keras.models.Model(input, output)\n",
        "    return model\n",
        "\n",
        "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
        "if len(physical_devices):\n",
        "    print(f'physical devices found: {physical_devices}')\n",
        "    mem_growth = tf.config.experimental.get_memory_growth(physical_devices[0])\n",
        "    print(f'memory growth of dev0: {mem_growth}')\n",
        "    if not mem_growth:\n",
        "        try:\n",
        "            tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
        "            print(f'memory growth of dev0: {tf.config.experimental.get_memory_growth(physical_devices[0])} (now enabled)')\n",
        "        except:\n",
        "            print(f'failed to modify device (likely already initialised)')\n",
        "else:\n",
        "    print('physical device not found')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Create function to make bottleneck version of CRT-Net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "gather": {
          "logged": 1715147719009
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "from sklearn import preprocessing\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from tensorflow.keras import layers, models, callbacks, utils\n",
        "import keras_nlp as nlp\n",
        "from tensorflow.keras.layers import LayerNormalization, Dense, Dropout, Add, Conv1D, MaxPooling1D, BatchNormalization, ReLU, Input, MultiHeadAttention\n",
        "import tensorflow as tf\n",
        "\n",
        "# Define the VGG block\n",
        "def vgg_block(input_tensor, filters):\n",
        "    x = Conv1D(filters, 3, padding='same', activation='relu')(input_tensor)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = ReLU()(x)\n",
        "    x = Conv1D(filters, 3, padding='same', activation='relu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = ReLU()(x)\n",
        "    x = MaxPooling1D(2, padding='same')(x)\n",
        "    return x\n",
        "\n",
        "# Define the Transformer encoder block\n",
        "def transformer_encoder_block(inputs, att_dim, att_heads, dropout_rate, ff_dim):\n",
        "    norm_input = LayerNormalization()(inputs)\n",
        "    attention_output = MultiHeadAttention(num_heads=att_heads, key_dim=att_dim)(norm_input, norm_input)\n",
        "    attention_output = Dropout(dropout_rate)(attention_output)\n",
        "    attention_output = Add()([inputs, attention_output])\n",
        "    attention_output = LayerNormalization()(attention_output)\n",
        "\n",
        "    ff_output = Dense(ff_dim, activation='relu')(attention_output)\n",
        "    ff_output = Dense(inputs.shape[-1])(ff_output)\n",
        "    ff_output = Dropout(dropout_rate)(ff_output)\n",
        "    ff_output = Add()([attention_output, ff_output])\n",
        "    return LayerNormalization()(ff_output)\n",
        "\n",
        "# Define the Bottleneck block\n",
        "def bottleneck_block(x, in_channels, out_channels, kernel_size, stride, downsample, use_bn, use_do):\n",
        "    identity = x\n",
        "\n",
        "    x = Conv1D(filters=out_channels, kernel_size=kernel_size, strides=stride, padding='same')(x)\n",
        "    if use_bn:\n",
        "        x = BatchNormalization()(x)\n",
        "    x = ReLU()(x)\n",
        "    if use_do:\n",
        "        x = Dropout(0.5)(x)\n",
        "\n",
        "    x = Conv1D(filters=out_channels, kernel_size=kernel_size, strides=1, padding='same')(x)\n",
        "    if use_bn:\n",
        "        x = BatchNormalization()(x)\n",
        "    x = ReLU()(x)\n",
        "    if use_do:\n",
        "        x = Dropout(0.5)(x)\n",
        "\n",
        "    if downsample:\n",
        "        identity = MaxPooling1D(pool_size=stride, padding='same')(identity)\n",
        "\n",
        "    if out_channels != in_channels:\n",
        "        identity = Conv1D(filters=out_channels, kernel_size=1, padding='same')(identity)\n",
        "\n",
        "    x = layers.add([x, identity])\n",
        "    return x\n",
        "\n",
        "# Model creation function\n",
        "def create_crtnet_bottleneck(number_of_leads=1,\n",
        "                   cnn_units=128,\n",
        "                   vgg_blocks=1,\n",
        "                   rnn_units=64,\n",
        "                   transformer_encoders=4,\n",
        "                   att_dim=64,\n",
        "                   att_heads=8,\n",
        "                   ff_dim=64,\n",
        "                   dropout_rate=0.1,\n",
        "                   num_classes=5, \n",
        "                   multilabel=False):\n",
        "    input = Input(shape=(None, number_of_leads))\n",
        "    x = input\n",
        "\n",
        "    for _ in range(vgg_blocks):\n",
        "        x = vgg_block(x, cnn_units)\n",
        "\n",
        "    # Add bottleneck blocks\n",
        "    x = bottleneck_block(x, in_channels=cnn_units, out_channels=cnn_units, kernel_size=3, stride=2, downsample=True, use_bn=True, use_do=True)\n",
        "\n",
        "    x = layers.Bidirectional(layers.GRU(rnn_units, return_sequences=True), merge_mode='sum')(x)\n",
        "\n",
        "    for _ in range(transformer_encoders):\n",
        "        x = transformer_encoder_block(x, att_dim, att_heads, dropout_rate, ff_dim)\n",
        "\n",
        "    x = layers.GlobalAveragePooling1D()(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "    x = Dense(ff_dim, activation='relu')(x)\n",
        "    x = Dense(ff_dim, activation='relu')(x)\n",
        "\n",
        "    if multilabel:\n",
        "        x = layers.Dense(num_classes, activation='sigmoid')(x)\n",
        "    else:\n",
        "        x = layers.Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    model = models.Model(inputs=input, outputs=x)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Load data from numpy files, then train and evaluate of whichever model, by default the baseline CRT-Net\n",
        "\n",
        "The loss function depends on whether doing multilabel or single label classification, which is inferred from the data (ie if each sample has exlusive one hot encoding or multiple)\n",
        "\n",
        "The class names are read to label the outputs more clearly based on SNOMED, vs just having unlabeled one-hot encoding position"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "gather": {
          "logged": 1715305394088
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            " 5/86 [>.............................] - ETA: 13:53 - loss: 2.7881 - accuracy: 0.1375"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[94], line 26\u001b[0m\n\u001b[1;32m     22\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_focal_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_multilabel \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     24\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39moptimizer, loss\u001b[38;5;241m=\u001b[39mloss, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 26\u001b[0m \u001b[43mtrain_and_evaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mone_hot_encoding_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mone_hot_encoding_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mreduce_lr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstopping\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclasses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclasses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultilabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_multilabel\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[92], line 11\u001b[0m, in \u001b[0;36mtrain_and_evaluate_model\u001b[0;34m(model, samples, one_hot_encoding_labels, classes, multilabel, callbacks, epochs, batch_size)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_and_evaluate_model\u001b[39m(model, samples, one_hot_encoding_labels, classes, multilabel, callbacks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m):\n\u001b[1;32m     10\u001b[0m     train_x, validation_x, train_y, validation_y \u001b[38;5;241m=\u001b[39m train_test_split(samples, one_hot_encoding_labels, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m     history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvalidation_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_y\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     pd\u001b[38;5;241m.\u001b[39mDataFrame(history\u001b[38;5;241m.\u001b[39mhistory)\u001b[38;5;241m.\u001b[39mplot(\n\u001b[1;32m     14\u001b[0m         figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m5\u001b[39m), xlim\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0\u001b[39m, epochs], ylim\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m], grid\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, xlabel\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     15\u001b[0m         style\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr--\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr--.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb-\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb-*\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     16\u001b[0m     plt\u001b[38;5;241m.\u001b[39mlegend(loc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlower left\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m~/anaconda3/envs/tensorflow210auto/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
            "File \u001b[0;32m~/anaconda3/envs/tensorflow210auto/lib/python3.10/site-packages/keras/engine/training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1556\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1558\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1561\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1562\u001b[0m ):\n\u001b[1;32m   1563\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1564\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1565\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1566\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
            "File \u001b[0;32m~/anaconda3/envs/tensorflow210auto/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
            "File \u001b[0;32m~/anaconda3/envs/tensorflow210auto/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
            "File \u001b[0;32m~/anaconda3/envs/tensorflow210auto/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateless_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
            "File \u001b[0;32m~/anaconda3/envs/tensorflow210auto/lib/python3.10/site-packages/tensorflow/python/eager/function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m   2494\u001b[0m   (graph_function,\n\u001b[1;32m   2495\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/tensorflow210auto/lib/python3.10/site-packages/tensorflow/python/eager/function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1865\u001b[0m     args,\n\u001b[1;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1867\u001b[0m     executing_eagerly)\n\u001b[1;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
            "File \u001b[0;32m~/anaconda3/envs/tensorflow210auto/lib/python3.10/site-packages/tensorflow/python/eager/function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
            "File \u001b[0;32m~/anaconda3/envs/tensorflow210auto/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import tensorflow.keras as keras\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import datetime\n",
        "import numpy as np\n",
        "    \n",
        "stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, min_lr=0.00001)\n",
        "\n",
        "# load the data from the file\n",
        "samples = np.load(os.path.join(\"data\", 'cpsc_2018_crtnet_samples.npy'))\n",
        "one_hot_encoding_labels = np.load(os.path.join(\"data\", 'cpsc_2018_crtnet_one_hot_encoding_labels.npy'))\n",
        "classes = np.load(os.path.join(\"data\", 'cpsc_2018_crtnet_classes.npy'))\n",
        "\n",
        "is_multilabel = any(sum(row) > 1 for row in one_hot_encoding_labels)\n",
        "\n",
        "model = create_crt_baseline(number_of_leads=samples.shape[2], num_classes=one_hot_encoding_labels.shape[1], multilabel=is_multilabel)\n",
        "\n",
        "initial_learning_rate = 0.0001\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=initial_learning_rate)\n",
        "\n",
        "loss = 'binary_focal_crossentropy' if is_multilabel else 'categorical_focal_crossentropy'\n",
        "\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
        "\n",
        "train_and_evaluate_model(model, samples=samples, one_hot_encoding_labels=one_hot_encoding_labels,callbacks=[reduce_lr, stopping], epochs=30, batch_size=64, classes=classes, multilabel=is_multilabel)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "tf215gpu"
    },
    "kernelspec": {
      "display_name": "tf215gpu",
      "language": "python",
      "name": "tf215gpu"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
