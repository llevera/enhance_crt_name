{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1715159522567
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import wfdb\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def read_records(record_files):\n",
        "    records = []\n",
        "    labels = []\n",
        "    for record_file in record_files:\n",
        "        record = wfdb.rdrecord(record_file)\n",
        "        if record.file_name[0].endswith('.dat'):\n",
        "            # TODO work out how to deal with MIT-BIH with its different hea/atr/dat files\n",
        "            # and very low samples. Split into many files?\n",
        "            ann = wfdb.rdann(record_file,'atr')\n",
        "        else:\n",
        "            for comment in record.comments:\n",
        "                if comment.startswith('Dx') or comment.startswith(' Dx'):\n",
        "                    dxs = set(arr.strip() for arr in comment.split(': ')[1].split(','))\n",
        "                    labels.append(dxs)\n",
        "                    \n",
        "        records.append(wfdb.rdrecord(record_file))\n",
        "    return records, labels\n",
        "\n",
        "\n",
        "def create_one_hot_labels(all_labels, target_classes, num_recordings):\n",
        "    discard_index = list()\n",
        "    labels = np.zeros((num_recordings, len(target_classes)))#, dtype=np.bool)\n",
        "    for i in range(num_recordings):\n",
        "        dxs = all_labels[i]\n",
        "        flag = np.zeros((1,len(dxs)), dtype = bool)\n",
        "        count = 0\n",
        "        for dx in dxs:\n",
        "            if dx in target_classes:\n",
        "                j = target_classes.index(dx)\n",
        "                labels[i, j] = 1\n",
        "                flag [0 ,count] = True\n",
        "\n",
        "            count += 1\n",
        "\n",
        "        # note any recordings that don't have any of the classes we are looking for\n",
        "        if np.any(flag) == False:\n",
        "            discard_index.append(i)\n",
        "\n",
        "    return labels, discard_index\n",
        "\n",
        "def get_unique_classes(all_labels, valid_classes=None):\n",
        "\n",
        "    classes2 = list()\n",
        "    for i in range(len(all_labels)):\n",
        "        dxs = all_labels[i]\n",
        "        for dx in dxs:\n",
        "            if valid_classes is None or dx in valid_classes:\n",
        "                classes2.append(dx)\n",
        "\n",
        "    classes3 = list()\n",
        "    for x in classes2:\n",
        "        if x not in classes3:\n",
        "            classes3.append(x)\n",
        "\n",
        "    classes3 = sorted (classes3)\n",
        "    return classes3\n",
        "\n",
        "def find_records(directory):\n",
        "    record_files = []\n",
        "    for dirpath, _, filenames in os.walk(directory):\n",
        "        for f in sorted(filenames):\n",
        "            file_path = os.path.join(dirpath, f)\n",
        "            if os.path.isfile(file_path) and not f.lower().startswith('.'):\n",
        "                file, ext = os.path.splitext(file_path)\n",
        "                if ext.lower() == '.hea':\n",
        "                    record_files.append(file)\n",
        "    if record_files:\n",
        "        return record_files\n",
        "    else:\n",
        "        raise IOError('No record files found.')\n",
        "\n",
        "def filter(data, labels, index):\n",
        "    labels = [labels[i] for i in range(len(labels)) if i not in index]\n",
        "    data = [data[i] for i in range(len(data)) if i not in index]\n",
        "    return labels, data\n",
        "\n",
        "def consolidate_equivalent_classes(one_hot_encoded_labels, unique_classes):\n",
        "    equivalent_classes_collection = [['713427006', '59118001'], ['284470004', '63593006'], ['427172004', '17338001']]\n",
        "\n",
        "    # For each set of equivalent class, use only one class as the representative class for the set and discard the other classes in the set.\n",
        "    # The label for the representative class is positive if any of the labels in the set is positive.\n",
        "    remove_classes = list()\n",
        "    remove_indices = list()\n",
        "    for equivalent_classes in equivalent_classes_collection:\n",
        "        equivalent_classes = [x for x in equivalent_classes if x in unique_classes]\n",
        "        if len(equivalent_classes)>1:\n",
        "            other_classes = equivalent_classes[1:]\n",
        "            equivalent_indices = [unique_classes.index(x) for x in equivalent_classes]\n",
        "            representative_index = equivalent_indices[0]\n",
        "            other_indices = equivalent_indices[1:]\n",
        "\n",
        "            one_hot_encoded_labels[:, representative_index] = np.any(one_hot_encoded_labels[:, equivalent_indices], axis=1)\n",
        "            remove_classes += other_classes\n",
        "            remove_indices += other_indices\n",
        "\n",
        "    for x in remove_classes:\n",
        "        unique_classes.remove(x)\n",
        "    one_hot_encoded_labels = np.delete(one_hot_encoded_labels, remove_indices, axis=1)\n",
        "\n",
        "    return one_hot_encoded_labels, unique_classes\n",
        "\n",
        "def set_labels_to_normal_if_none_other(labels, unique_classes, normal_class):\n",
        "    # If the labels are negative for all classes, then change the label for the normal class to positive.\n",
        "    normal_index = unique_classes.index(normal_class)\n",
        "    for i in range(len(labels)):\n",
        "        num_positive_classes = np.sum(labels[i, :])\n",
        "        if num_positive_classes==0:\n",
        "            labels[i, normal_index] = 1\n",
        "\n",
        "    return labels\n",
        "\n",
        "def ensure_normal_class(unique_classes, normal_class):\n",
        "    if normal_class not in unique_classes:\n",
        "        unique_classes.add(normal_class)\n",
        "        print('- The normal class {} is not one of the label classes, so it has been automatically added, but please check that you chose the correct normal class.'.format(normal_class))\n",
        "    unique_classes = sorted(unique_classes)\n",
        "    return unique_classes\n",
        "\n",
        "def read_scored_classes():\n",
        "    scored = list()\n",
        "    with open('dx_mapping_scored.csv', 'r') as f:\n",
        "        for l in f:\n",
        "            dxs = (l.split(','))\n",
        "            scored.append(dxs[1])\n",
        "    return (sorted(scored[1:]))\n",
        "\n",
        "def filter_out(one_hot_encoded_labels, records, discard_index):\n",
        "    one_hot_encoded_labels = [one_hot_encoded_labels[i] for i in range(len(one_hot_encoded_labels)) if i not in discard_index]\n",
        "    records = [records[i] for i in range(len(records)) if i not in discard_index]\n",
        "\n",
        "    return one_hot_encoded_labels, records\n",
        "\n",
        "def load_records(record_file_list, adjust_classes_for_physionet, normal_class):\n",
        "        \n",
        "    if len(record_file_list) == 0:\n",
        "        raise ValueError('No record files found.')\n",
        "\n",
        "    num_recordings = len(record_file_list)\n",
        "\n",
        "    records, all_labels = read_records(record_file_list)\n",
        "\n",
        "    scored = None\n",
        "    if adjust_classes_for_physionet:\n",
        "        scored = read_scored_classes()\n",
        "\n",
        "    unique_classes = get_unique_classes(all_labels, scored)\n",
        "\n",
        "    if (normal_class is not None):\n",
        "        unique_classes = ensure_normal_class(unique_classes, normal_class)\n",
        "    \n",
        "    one_hot_encoded_labels, discard_index = create_one_hot_labels(all_labels, unique_classes, num_recordings)\n",
        "\n",
        "    if (adjust_classes_for_physionet):\n",
        "        one_hot_encoded_labels, unique_classes = consolidate_equivalent_classes(one_hot_encoded_labels, unique_classes)\n",
        "    \n",
        "    if (normal_class is not None):\n",
        "        one_hot_encoded_labels = set_labels_to_normal_if_none_other(one_hot_encoded_labels, unique_classes, normal_class)\n",
        "    \n",
        "    one_hot_encoded_labels, records = filter_out(one_hot_encoded_labels, records, discard_index)\n",
        "\n",
        "    return one_hot_encoded_labels, records, unique_classes\n",
        "\n",
        "def standardise_length(data, target_length):\n",
        "    number_of_leads = data.shape[0]\n",
        "    \n",
        "    if len(data[0])<=target_length:\n",
        "        ext= np.zeros([number_of_leads,target_length])\n",
        "        for i in range(0,number_of_leads):\n",
        "            ext[i][0:len(data[i])]=data[i]\n",
        "        return ext.T  \n",
        "    else:\n",
        "        cut = np.zeros([number_of_leads,target_length])\n",
        "        for i in range(number_of_leads):\n",
        "            tocut = len(data[0])- target_length\n",
        "            cut[i] = data[i][tocut:]\n",
        "        return cut.T \n",
        "\n",
        "def resample(data, src_frq, trg_frq):\n",
        "\n",
        "    if src_frq == trg_frq:\n",
        "        return data\n",
        "\n",
        "    N_src = data.shape[0]\n",
        "    N_trg = int(N_src * trg_frq / src_frq)\n",
        "    \n",
        "    resampled = np.zeros((N_trg, data.shape[1]), dtype='float32')\n",
        "    for i in range(data.shape[1]):\n",
        "        resampled[:,i] = np.interp(np.linspace(0, N_src, N_trg), np.arange(N_src), data[:, i])\n",
        "        \n",
        "    return resampled\n",
        "\n",
        "def standardise_data_samples(records):\n",
        "    standardised_data = list()\n",
        "\n",
        "    # find the most common fs\n",
        "    fss = [record.fs for record in records]\n",
        "    target_fs = max(set(fss), key=fss.count)\n",
        "\n",
        "    # find the most common sig_len\n",
        "    sig_lens = [record.sig_len for record in records]\n",
        "    target_length = max(set(sig_lens), key=sig_lens.count)\n",
        "\n",
        "    for i in range(len(records)):\n",
        "        datum = records[i].p_signal.T\n",
        "        datum = resample(datum,records[i].fs, target_fs)\n",
        "        datum = standardise_length(datum, target_length)\n",
        "        standardised_data.append(datum)\n",
        "\n",
        "    return standardised_data\n",
        "\n",
        "def load_data(input_directory, adjust_classes_for_physionet=False, normal_class=None):\n",
        "    record_file_list = find_records(input_directory)\n",
        "    \n",
        "    one_hot_encoded_labels, records, classes = load_records(record_file_list, adjust_classes_for_physionet, normal_class=normal_class)\n",
        "    samples = standardise_data_samples(records)\n",
        "            \n",
        "    one_hot_encoded_labels = np.stack(one_hot_encoded_labels, axis =0)\n",
        "    samples = np.stack(samples, axis =0)\n",
        "\n",
        "    return one_hot_encoded_labels, samples, classes\n",
        "\n",
        "output_directory = 'data'\n",
        "input_directory = 'training_data/cpsc_2018_subset'\n",
        "    \n",
        "one_hot_encoding_labels, samples, classes = load_data(input_directory)\n",
        "\n",
        "# save the data to a file\n",
        "np.save(os.path.join(output_directory, 'samples.npy'), samples)\n",
        "np.save(os.path.join(output_directory, 'one_hot_encoding_labels.npy'), one_hot_encoding_labels)\n",
        "np.save(os.path.join(output_directory, 'classes.npy'), classes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "gather": {
          "logged": 1715145670437
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "physical device not found\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras import layers\n",
        "import keras_nlp as nlp \n",
        "import tensorflow.keras as keras\n",
        "\n",
        "\n",
        "def train_and_evaluate_model(model, samples, one_hot_encoding_labels, callbacks=None, epochs=10, batch_size=64, classes=None):\n",
        "    \n",
        "    train_x, validation_x, train_y, validation_y = train_test_split(samples, one_hot_encoding_labels, test_size=0.2, random_state=42)\n",
        "    history = model.fit(train_x, train_y, epochs=epochs, batch_size=batch_size, validation_data=(validation_x, validation_y), callbacks=callbacks)\n",
        "\n",
        "    pd.DataFrame(history.history).plot(\n",
        "        figsize=(8, 5), xlim=[0, epochs], ylim=[0, 1], grid=True, xlabel=\"Epoch\",\n",
        "        style=[\"r--\", \"r--.\", \"b-\", \"b-*\"])\n",
        "    plt.legend(loc=\"lower left\")\n",
        "    plt.show()\n",
        "\n",
        "    y_pred = model.predict(validation_x)\n",
        "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "    validation_y = np.argmax(validation_y, axis=1)\n",
        "\n",
        "    # Confusion Matrix\n",
        "    cm = confusion_matrix(validation_y, y_pred_classes)\n",
        "\n",
        "    # Classification Report\n",
        "    if classes is None:\n",
        "        classes = [\"Class \" + str(i) for i in range(len(np.unique(validation_y)))]\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=classes, yticklabels=classes)\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.show()\n",
        "\n",
        "def vgg_block(input, cnn_units):\n",
        "    output = input\n",
        "    output = layers.Conv1D(cnn_units, 3, padding='same', activation='relu')(output)\n",
        "    output = layers.BatchNormalization()(output)\n",
        "    output = layers.Dropout(0.1)(output)\n",
        "    output = layers.Conv1D(cnn_units, 3, padding='same', activation='relu')(output)\n",
        "    output = layers.BatchNormalization()(output)\n",
        "    output = layers.Dropout(0.1)(output)\n",
        "    output = layers.Conv1D(cnn_units, 24, padding='same', activation='relu', strides=2)(output)\n",
        "    output = layers.BatchNormalization()(output)\n",
        "    output = layers.MaxPooling1D(2, padding='same')(output)\n",
        "    return output\n",
        "\n",
        "def create_crt_baseline(number_of_leads=None, cnn_units=128, vgg_blocks=3, rnn_units=64,\n",
        "                   transformer_encoders=4, att_dim=64, att_heads=8, fnn_units=64, num_classes=5):\n",
        "    input = layers.Input(shape=(None, number_of_leads))\n",
        "    output = input\n",
        "\n",
        "    for _ in range(vgg_blocks):\n",
        "        output = layers.BatchNormalization()(output)\n",
        "        output = vgg_block(output, cnn_units)\n",
        "\n",
        "    output = layers.BatchNormalization()(output)\n",
        "    output = layers.Bidirectional(layers.GRU(rnn_units, return_sequences=True), merge_mode='sum')(output)\n",
        "    output = layers.BatchNormalization()(output)\n",
        "\n",
        "    if transformer_encoders > 0:\n",
        "        output = output + nlp.layers.SinePositionEncoding(max_wavelength=10000)(output)\n",
        "\n",
        "        for _ in range(transformer_encoders):\n",
        "            output = layers.BatchNormalization()(output)\n",
        "            output = nlp.layers.TransformerEncoder(att_dim, att_heads)(output)\n",
        "\n",
        "        output = layers.GlobalAveragePooling1D()(output)\n",
        "\n",
        "    output = layers.Dropout(0.2)(output)\n",
        "    output = layers.Dense(fnn_units, activation='relu')(output)\n",
        "    output = layers.BatchNormalization()(output)\n",
        "    output = layers.Dropout(0.2)(output)\n",
        "    output = layers.Dense(fnn_units, activation='relu')(output)\n",
        "    output = layers.BatchNormalization()(output)\n",
        "    output = layers.Dense(num_classes, activation='sigmoid')(output)\n",
        "    model = keras.models.Model(input, output)\n",
        "    return model\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
        "if len(physical_devices):\n",
        "    print(f'physical devices found: {physical_devices}')\n",
        "    mem_growth = tf.config.experimental.get_memory_growth(physical_devices[0])\n",
        "    print(f'memory growth of dev0: {mem_growth}')\n",
        "    if not mem_growth:\n",
        "        try:\n",
        "            tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
        "            print(f'memory growth of dev0: {tf.config.experimental.get_memory_growth(physical_devices[0])} (now enabled)')\n",
        "        except:\n",
        "            print(f'failed to modify device (likely already initialised)')\n",
        "else:\n",
        "    print('physical device not found')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1715147719009
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "from sklearn import preprocessing\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from tensorflow.keras import layers, models, callbacks, utils\n",
        "import keras_nlp as nlp\n",
        "from tensorflow.keras.layers import LayerNormalization, Dense, Dropout, Add, Conv1D, MaxPooling1D, BatchNormalization, ReLU, Input, MultiHeadAttention\n",
        "import tensorflow as tf\n",
        "\n",
        "# Define the VGG block\n",
        "def vgg_block(input_tensor, filters):\n",
        "    x = Conv1D(filters, 3, padding='same', activation='relu')(input_tensor)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = ReLU()(x)\n",
        "    x = Conv1D(filters, 3, padding='same', activation='relu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = ReLU()(x)\n",
        "    x = MaxPooling1D(2, padding='same')(x)\n",
        "    return x\n",
        "\n",
        "# Define the Transformer encoder block\n",
        "def transformer_encoder_block(inputs, att_dim, att_heads, dropout_rate, ff_dim):\n",
        "    norm_input = LayerNormalization()(inputs)\n",
        "    attention_output = MultiHeadAttention(num_heads=att_heads, key_dim=att_dim)(norm_input, norm_input)\n",
        "    attention_output = Dropout(dropout_rate)(attention_output)\n",
        "    attention_output = Add()([inputs, attention_output])\n",
        "    attention_output = LayerNormalization()(attention_output)\n",
        "\n",
        "    ff_output = Dense(ff_dim, activation='relu')(attention_output)\n",
        "    ff_output = Dense(inputs.shape[-1])(ff_output)\n",
        "    ff_output = Dropout(dropout_rate)(ff_output)\n",
        "    ff_output = Add()([attention_output, ff_output])\n",
        "    return LayerNormalization()(ff_output)\n",
        "\n",
        "# Define the Bottleneck block\n",
        "def bottleneck_block(x, in_channels, out_channels, kernel_size, stride, downsample, use_bn, use_do):\n",
        "    identity = x\n",
        "\n",
        "    x = Conv1D(filters=out_channels, kernel_size=kernel_size, strides=stride, padding='same')(x)\n",
        "    if use_bn:\n",
        "        x = BatchNormalization()(x)\n",
        "    x = ReLU()(x)\n",
        "    if use_do:\n",
        "        x = Dropout(0.5)(x)\n",
        "\n",
        "    x = Conv1D(filters=out_channels, kernel_size=kernel_size, strides=1, padding='same')(x)\n",
        "    if use_bn:\n",
        "        x = BatchNormalization()(x)\n",
        "    x = ReLU()(x)\n",
        "    if use_do:\n",
        "        x = Dropout(0.5)(x)\n",
        "\n",
        "    if downsample:\n",
        "        identity = MaxPooling1D(pool_size=stride, padding='same')(identity)\n",
        "\n",
        "    if out_channels != in_channels:\n",
        "        identity = Conv1D(filters=out_channels, kernel_size=1, padding='same')(identity)\n",
        "\n",
        "    x = layers.add([x, identity])\n",
        "    return x\n",
        "\n",
        "# Model creation function\n",
        "def create_crtnet_bottleneck(number_of_leads=1,\n",
        "                   cnn_units=128,\n",
        "                   vgg_blocks=1,\n",
        "                   rnn_units=64,\n",
        "                   transformer_encoders=4,\n",
        "                   att_dim=64,\n",
        "                   att_heads=8,\n",
        "                   ff_dim=64,\n",
        "                   dropout_rate=0.1,\n",
        "                   num_classes=5):\n",
        "    input = Input(shape=(None, number_of_leads))\n",
        "    x = input\n",
        "\n",
        "    for _ in range(vgg_blocks):\n",
        "        x = vgg_block(x, cnn_units)\n",
        "\n",
        "    # Add bottleneck blocks\n",
        "    x = bottleneck_block(x, in_channels=cnn_units, out_channels=cnn_units, kernel_size=3, stride=2, downsample=True, use_bn=True, use_do=True)\n",
        "\n",
        "    x = layers.Bidirectional(layers.GRU(rnn_units, return_sequences=True), merge_mode='sum')(x)\n",
        "\n",
        "    for _ in range(transformer_encoders):\n",
        "        x = transformer_encoder_block(x, att_dim, att_heads, dropout_rate, ff_dim)\n",
        "\n",
        "    x = layers.GlobalAveragePooling1D()(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "    x = Dense(ff_dim, activation='relu')(x)\n",
        "    x = Dense(ff_dim, activation='relu')(x)\n",
        "    x = Dense(num_classes, activation='sigmoid')(x)\n",
        "\n",
        "    model = models.Model(inputs=input, outputs=x)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "gather": {
          "logged": 1715158341711
        }
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[10], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m one_hot_encoding_labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mone_hot_encoding_labels.npy\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     15\u001b[0m classes \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclasses.npy\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m---> 17\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_crt_baseline\u001b[49m(number_of_leads\u001b[38;5;241m=\u001b[39msamples\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m], num_classes\u001b[38;5;241m=\u001b[39mone_hot_encoding_labels\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     19\u001b[0m initial_learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0001\u001b[39m\n\u001b[1;32m     20\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39minitial_learning_rate)\n",
            "Cell \u001b[0;32mIn[10], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m one_hot_encoding_labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mone_hot_encoding_labels.npy\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     15\u001b[0m classes \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclasses.npy\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m---> 17\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_crt_baseline\u001b[49m(number_of_leads\u001b[38;5;241m=\u001b[39msamples\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m], num_classes\u001b[38;5;241m=\u001b[39mone_hot_encoding_labels\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     19\u001b[0m initial_learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0001\u001b[39m\n\u001b[1;32m     20\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39minitial_learning_rate)\n",
            "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1457\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:701\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1152\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1135\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:312\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32m~/anaconda3/envs/tensorflow210auto/lib/python3.10/site-packages/debugpy/_vendored/pydevd/pydevd.py:2070\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[1;32m   2067\u001b[0m             from_this_thread\u001b[38;5;241m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[1;32m   2069\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads_suspended_single_notification\u001b[38;5;241m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[0;32m-> 2070\u001b[0m         keep_suspended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuspend_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_this_thread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_tracker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2072\u001b[0m frames_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2074\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[1;32m   2075\u001b[0m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/envs/tensorflow210auto/lib/python3.10/site-packages/debugpy/_vendored/pydevd/pydevd.py:2106\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[1;32m   2103\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_input_hook()\n\u001b[1;32m   2105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_internal_commands()\n\u001b[0;32m-> 2106\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m(frame)))\n\u001b[1;32m   2110\u001b[0m \u001b[38;5;66;03m# process any stepping instructions\u001b[39;00m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import tensorflow.keras as keras\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import datetime\n",
        "import numpy as np\n",
        "    \n",
        "stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, min_lr=0.00001)\n",
        "    \n",
        "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
        "\n",
        "# load the data from the file\n",
        "samples = np.load(os.path.join(\"data\", 'samples.npy'))\n",
        "one_hot_encoding_labels = np.load(os.path.join(\"data\", 'one_hot_encoding_labels.npy'))\n",
        "classes = np.load(os.path.join(\"data\", 'classes.npy'))\n",
        "\n",
        "model = create_crt_baseline(number_of_leads=samples.shape[2], num_classes=one_hot_encoding_labels.shape[1])\n",
        "\n",
        "initial_learning_rate = 0.0001\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=initial_learning_rate)\n",
        "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "train_and_evaluate_model(model, samples=samples, one_hot_encoding_labels=one_hot_encoding_labels,callbacks=[reduce_lr, stopping], epochs=1, batch_size=64, classes=classes)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "tf215gpu"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
