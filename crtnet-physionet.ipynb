{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Howto\n",
        "\n",
        "\n",
        "1. Decide if wanting to process the data and evaluate the same as CRT-Net paper, ie allow whatever label classes are in the data, but for each sample, just pick the first diagnosis OR do it PhysioNet competition style where only samples from a certain number of labels (as per csv file) are used but also each sample can have multiple labels\n",
        "1. If you want CRT-Net style, run the cell where data is loaded with load_data given the argument adjust_classes_for_physionet as false. This will produce three file, the data samples themselves, the one hot encoded labels (only one per sample) and the discovered class names, to make it easier later to translate on hot labels to readable class names\n",
        "1. If you want PhysioNet style, run cell with load_data given the argument adjust_classes_for_physionet as true\n",
        "1. Run the cell that creates the train/evaluate functions plus creates the baseline crt_net model\n",
        "1. Run other cells to create other models\n",
        "1. Change the cell to read whatever numpy datafiles you like, and train/evaluate. It will work out whether to do multilabel or single label based on the onehot encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "\n",
        "The below reads .hea and .mat file pairs and then :\n",
        "\n",
        "- Standardises the sample lengths to the most common length (unless overridden), and sampling frequency to the most common frequency.\n",
        "- One hot encodes the diagnostic class label, either picking the first \"primary\" label, or encoding all labels. The first case is consistency with the CRT-Net paper, the second for consistency with PhysioNet challenge\n",
        "- To also be consistent with the PhysioNet challenge, it is also catered to restrict the classes to a certain specified set (via csv file), and also to ensure the \"normal\" class is always present\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1715416540227
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import wfdb\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def read_records(record_files):\n",
        "    records = []\n",
        "    labels = []\n",
        "    for record_file in record_files:\n",
        "        record = wfdb.rdrecord(record_file)\n",
        "        if record.file_name[0].endswith('.dat'):\n",
        "            # TODO work out how to deal with MIT-BIH with its different hea/atr/dat files\n",
        "            # and very low samples. Split into many files?\n",
        "            ann = wfdb.rdann(record_file,'atr')\n",
        "        else:\n",
        "            for comment in record.comments:\n",
        "                if comment.startswith('Dx') or comment.startswith(' Dx'):\n",
        "                    dxs = set(arr.strip() for arr in comment.split(': ')[1].split(','))\n",
        "                    labels.append(dxs)\n",
        "                    \n",
        "        records.append(wfdb.rdrecord(record_file))\n",
        "    return records, labels\n",
        "\n",
        "\n",
        "def create_one_hot_labels(all_labels, target_classes, num_recordings, adjust_classes_for_physionet):\n",
        "    discard_index = list()\n",
        "    labels = np.zeros((num_recordings, len(target_classes)))  # , dtype=np.bool)\n",
        "    for i in range(num_recordings):\n",
        "        dxs = all_labels[i]\n",
        "        flag = np.zeros((1, len(dxs)), dtype=bool)\n",
        "        count = 0\n",
        "        for dx in dxs:\n",
        "            if dx in target_classes:\n",
        "                j = target_classes.index(dx)\n",
        "                labels[i, j] = 1\n",
        "                flag[0, count] = True\n",
        "                \n",
        "                # Break out of the loop if adjust_classes_for_physionet is not set\n",
        "                if not adjust_classes_for_physionet:\n",
        "                    break\n",
        "\n",
        "            count += 1\n",
        "\n",
        "        # note any recordings that don't have any of the classes we are looking for\n",
        "        if np.any(flag) == False:\n",
        "            discard_index.append(i)\n",
        "\n",
        "    return labels, discard_index\n",
        "\n",
        "def get_unique_classes(all_labels, valid_classes=None):\n",
        "\n",
        "    classes2 = list()\n",
        "    for i in range(len(all_labels)):\n",
        "        dxs = all_labels[i]\n",
        "        for dx in dxs:\n",
        "            if valid_classes is None or dx in valid_classes:\n",
        "                classes2.append(dx)\n",
        "\n",
        "    classes3 = list()\n",
        "    for x in classes2:\n",
        "        if x not in classes3:\n",
        "            classes3.append(x)\n",
        "\n",
        "    classes3 = sorted (classes3)\n",
        "    return classes3\n",
        "\n",
        "def find_records(directory):\n",
        "    record_files = []\n",
        "    for dirpath, _, filenames in os.walk(directory):\n",
        "        for f in sorted(filenames):\n",
        "            file_path = os.path.join(dirpath, f)\n",
        "            if os.path.isfile(file_path) and not f.lower().startswith('.'):\n",
        "                file, ext = os.path.splitext(file_path)\n",
        "                if ext.lower() == '.hea':\n",
        "                    record_files.append(file)\n",
        "    if record_files:\n",
        "        return record_files\n",
        "    else:\n",
        "        raise IOError('No record files found.')\n",
        "\n",
        "def filter(data, labels, index):\n",
        "    labels = [labels[i] for i in range(len(labels)) if i not in index]\n",
        "    data = [data[i] for i in range(len(data)) if i not in index]\n",
        "    return labels, data\n",
        "\n",
        "def consolidate_equivalent_classes(one_hot_encoded_labels, unique_classes):\n",
        "    equivalent_classes_collection = [['713427006', '59118001'], ['284470004', '63593006'], ['427172004', '17338001']]\n",
        "\n",
        "    # For each set of equivalent class, use only one class as the representative class for the set and discard the other classes in the set.\n",
        "    # The label for the representative class is positive if any of the labels in the set is positive.\n",
        "    remove_classes = list()\n",
        "    remove_indices = list()\n",
        "    for equivalent_classes in equivalent_classes_collection:\n",
        "        equivalent_classes = [x for x in equivalent_classes if x in unique_classes]\n",
        "        if len(equivalent_classes)>1:\n",
        "            other_classes = equivalent_classes[1:]\n",
        "            equivalent_indices = [unique_classes.index(x) for x in equivalent_classes]\n",
        "            representative_index = equivalent_indices[0]\n",
        "            other_indices = equivalent_indices[1:]\n",
        "\n",
        "            one_hot_encoded_labels[:, representative_index] = np.any(one_hot_encoded_labels[:, equivalent_indices], axis=1)\n",
        "            remove_classes += other_classes\n",
        "            remove_indices += other_indices\n",
        "\n",
        "    for x in remove_classes:\n",
        "        unique_classes.remove(x)\n",
        "    one_hot_encoded_labels = np.delete(one_hot_encoded_labels, remove_indices, axis=1)\n",
        "\n",
        "    return one_hot_encoded_labels, unique_classes\n",
        "\n",
        "def set_labels_to_normal_if_none_other(labels, unique_classes, normal_class):\n",
        "    # If the labels are negative for all classes, then change the label for the normal class to positive.\n",
        "    normal_index = unique_classes.index(normal_class)\n",
        "    for i in range(len(labels)):\n",
        "        num_positive_classes = np.sum(labels[i, :])\n",
        "        if num_positive_classes==0:\n",
        "            labels[i, normal_index] = 1\n",
        "\n",
        "    return labels\n",
        "\n",
        "def ensure_normal_class(unique_classes, normal_class):\n",
        "    if normal_class not in unique_classes:\n",
        "        unique_classes.add(normal_class)\n",
        "        print('- The normal class {} is not one of the label classes, so it has been automatically added, but please check that you chose the correct normal class.'.format(normal_class))\n",
        "    unique_classes = sorted(unique_classes)\n",
        "    return unique_classes\n",
        "\n",
        "def read_scored_classes():\n",
        "    scored = list()\n",
        "    with open('dx_mapping_scored.csv', 'r') as f:\n",
        "        for l in f:\n",
        "            dxs = (l.split(','))\n",
        "            scored.append(dxs[1])\n",
        "    return (sorted(scored[1:]))\n",
        "\n",
        "def filter_out(one_hot_encoded_labels, records, discard_index):\n",
        "    one_hot_encoded_labels = [one_hot_encoded_labels[i] for i in range(len(one_hot_encoded_labels)) if i not in discard_index]\n",
        "    records = [records[i] for i in range(len(records)) if i not in discard_index]\n",
        "\n",
        "    return one_hot_encoded_labels, records\n",
        "\n",
        "def load_records(record_file_list, adjust_classes_for_physionet, normal_class):\n",
        "        \n",
        "    if len(record_file_list) == 0:\n",
        "        raise ValueError('No record files found.')\n",
        "\n",
        "    num_recordings = len(record_file_list)\n",
        "\n",
        "    records, all_labels = read_records(record_file_list)\n",
        "\n",
        "    scored = None\n",
        "    if adjust_classes_for_physionet:\n",
        "        scored = read_scored_classes()\n",
        "\n",
        "    unique_classes = get_unique_classes(all_labels, scored)\n",
        "\n",
        "    if (normal_class is not None):\n",
        "        unique_classes = ensure_normal_class(unique_classes, normal_class)\n",
        "    \n",
        "    one_hot_encoded_labels, discard_index = create_one_hot_labels(all_labels, unique_classes, num_recordings, adjust_classes_for_physionet=adjust_classes_for_physionet)\n",
        "\n",
        "    if (adjust_classes_for_physionet):\n",
        "        one_hot_encoded_labels, unique_classes = consolidate_equivalent_classes(one_hot_encoded_labels, unique_classes)\n",
        "    \n",
        "    if (normal_class is not None):\n",
        "        one_hot_encoded_labels = set_labels_to_normal_if_none_other(one_hot_encoded_labels, unique_classes, normal_class)\n",
        "    \n",
        "    one_hot_encoded_labels, records = filter_out(one_hot_encoded_labels, records, discard_index)\n",
        "\n",
        "    return one_hot_encoded_labels, records, unique_classes\n",
        "\n",
        "def standardise_length(data, target_length):\n",
        "    number_of_leads = data.shape[0]\n",
        "    \n",
        "    if len(data[0])<=target_length:\n",
        "        ext= np.zeros([number_of_leads,target_length])\n",
        "        for i in range(0,number_of_leads):\n",
        "            ext[i][0:len(data[i])]=data[i]\n",
        "        return ext.T  \n",
        "    else:\n",
        "        cut = np.zeros([number_of_leads,target_length])\n",
        "        for i in range(number_of_leads):\n",
        "            tocut = len(data[0])- target_length\n",
        "            cut[i] = data[i][tocut:]\n",
        "        return cut.T \n",
        "\n",
        "def resample(data, src_frq, trg_frq):\n",
        "\n",
        "    if src_frq == trg_frq:\n",
        "        return data\n",
        "\n",
        "    N_src = data.shape[0]\n",
        "    N_trg = int(N_src * trg_frq / src_frq)\n",
        "    \n",
        "    resampled = np.zeros((N_trg, data.shape[1]), dtype='float32')\n",
        "    for i in range(data.shape[1]):\n",
        "        resampled[:,i] = np.interp(np.linspace(0, N_src, N_trg), np.arange(N_src), data[:, i])\n",
        "        \n",
        "    return resampled\n",
        "\n",
        "def standardise_data_samples(records, fixed_sample_length=None):\n",
        "    standardised_data = list()\n",
        "\n",
        "    # find the most common fs\n",
        "    fss = [record.fs for record in records]\n",
        "\n",
        "    target_fs = max(set(fss), key=fss.count)\n",
        "\n",
        "    # find the most common sig_len\n",
        "    sig_lens = [record.sig_len for record in records]\n",
        "\n",
        "    if (fixed_sample_length is not None):\n",
        "        target_length = fixed_sample_length\n",
        "    else:\n",
        "        target_length = max(set(sig_lens), key=sig_lens.count)\n",
        "\n",
        "    for i in range(len(records)):\n",
        "        datum = records[i].p_signal.T\n",
        "        datum = resample(datum,records[i].fs, target_fs)\n",
        "        datum = standardise_length(datum, target_length)\n",
        "        standardised_data.append(datum)\n",
        "\n",
        "    return standardised_data\n",
        "\n",
        "def load_data(input_directory, adjust_classes_for_physionet=False, normal_class=None, fixed_sample_length=None):\n",
        "    record_file_list = find_records(input_directory)\n",
        "    \n",
        "    one_hot_encoded_labels, records, classes = load_records(record_file_list, adjust_classes_for_physionet, normal_class=normal_class)\n",
        "    samples = standardise_data_samples(records, fixed_sample_length=fixed_sample_length)\n",
        "            \n",
        "    one_hot_encoded_labels = np.stack(one_hot_encoded_labels, axis =0)\n",
        "    samples = np.stack(samples, axis =0)\n",
        "\n",
        "    return one_hot_encoded_labels, samples, classes\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Run below to get an numpy output of samples, single one hot encoded label and the class names for the China 2018 data as used by the CRT-Net paper."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1715417107323
        }
      },
      "outputs": [],
      "source": [
        "output_directory = 'data'\n",
        "input_directory = 'cpsc_2018'\n",
        "    \n",
        "one_hot_encoding_labels, samples, classes = load_data(input_directory, adjust_classes_for_physionet=False, fixed_sample_length=3000)\n",
        "\n",
        "# maps SNOMED CT codes to classes\n",
        "snomed_to_abbreviation = {\n",
        "    '164884008': 'PVC',\n",
        "    '164889003': 'AF',\n",
        "    '164909002': 'LBBB',\n",
        "    '164931005': 'STE',\n",
        "    '270492004': 'IAVB',\n",
        "    '284470004': 'PAC',\n",
        "    '426783006': 'N',\n",
        "    '429622005': 'STD',\n",
        "    '59118001': 'RBBB'\n",
        "}\n",
        "\n",
        "# map classes to abbreviations\n",
        "mapped_abbreviations = [snomed_to_abbreviation.get(code, None) for code in classes]\n",
        "\n",
        "# save the data to a file\n",
        "np.save(os.path.join(output_directory, 'cpsc_2018_crtnet_samples.npy'), samples)\n",
        "np.save(os.path.join(output_directory, 'cpsc_2018_crtnet_one_hot_encoding_labels.npy'), one_hot_encoding_labels)\n",
        "np.save(os.path.join(output_directory, 'cpsc_2018_crtnet_classes.npy'), mapped_abbreviations)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Run below to get an numpy output of samples, multilabel one hot encoded labels and the class names for any PhysioNet data. This data will be restricted to the PhysioNet challenge list of classes and will be guaranteed to include the \"normal\" class.\n",
        "\n",
        "The can be run on any set of sets of PhysioNet data, including China 2018, PTB-XL etc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1715304125386
        }
      },
      "outputs": [],
      "source": [
        "output_directory = 'data'\n",
        "input_directory = 'cpsc_2018'\n",
        "    \n",
        "one_hot_encoding_labels, samples, classes = load_data(input_directory, adjust_classes_for_physionet=True, normal_class='426783006')\n",
        "# save the data to a file\n",
        "np.save(os.path.join(output_directory, 'cpsc_2018_physionet_samples.npy'), samples)\n",
        "np.save(os.path.join(output_directory, 'cpsc_2018_physionet_one_hot_encoding_labels.npy'), one_hot_encoding_labels)\n",
        "np.save(os.path.join(output_directory, 'cpsc_2018_physionet_classes.npy'), classes)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Create functions to \n",
        "a) Train and evaluate a model, be it multilabel or single label\n",
        "b) Create our best attempt at the matching architecture, regularisation and hyperparameters for CRT-Net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1715457039879
        }
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers\n",
        "import keras_nlp as nlp \n",
        "import tensorflow.keras as keras\n",
        "from sklearn.metrics import classification_report, multilabel_confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from numpy import argmax\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from importlib import reload\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from src import crtnet_models\n",
        "\n",
        "reload(crtnet_models)\n",
        "\n",
        "\n",
        "def train_and_evaluate_model(model, samples, one_hot_encoding_labels, classes, multilabel, callbacks=None, epochs=10, batch_size=64, style=None):\n",
        "    \n",
        "    train_x, validation_x, train_y, validation_y = train_test_split(samples, one_hot_encoding_labels, test_size=0.1, random_state=42)\n",
        "    history = model.fit(train_x, train_y, epochs=epochs, batch_size=batch_size, validation_data=(validation_x, validation_y), callbacks=callbacks)\n",
        "\n",
        "    pd.DataFrame(history.history).plot(\n",
        "        figsize=(8, 5), xlim=[0, epochs], ylim=[0, 1], grid=True, xlabel=\"Epoch\",\n",
        "        style=[\"r--\", \"r--.\", \"b-\", \"b-*\"] if style is None else style)\n",
        "    plt.legend(loc=\"lower left\")\n",
        "    plt.show()\n",
        "        \n",
        "    # Classification Report\n",
        "    if classes is None:\n",
        "        classes = [\"Class \" + str(i) for i in range(len(np.unique(validation_y)))]\n",
        "\n",
        "    y_pred = model.predict(validation_x)\n",
        "\n",
        "    if (multilabel):\n",
        "        y_pred = (y_pred > 0.5).astype(int)\n",
        "\n",
        "        report = classification_report(validation_y, y_pred,  labels=classes, zero_division=0)\n",
        "        print(report)\n",
        "\n",
        "    else:\n",
        "        validation_y = np.argmax(validation_y, axis=1)\n",
        "        y_pred = np.argmax(y_pred, axis=1)\n",
        "\n",
        "        report = classification_report(y_true=validation_y, y_pred=y_pred, labels=np.unique(validation_y), target_names=classes,zero_division=0)\n",
        "        print(report)\n",
        "\n",
        "\n",
        "        # output accuracy per class\n",
        "        print('Accuracy per class:')\n",
        "        for i in range(len(classes)):\n",
        "            print(f'{classes[i]}: {np.round(100*sum(validation_y[validation_y==i] == y_pred[validation_y==i])/sum(validation_y==i), 2)}%')\n",
        "\n",
        "        # Confusion Matrix\n",
        "        cm = confusion_matrix(validation_y, y_pred)\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        sns.heatmap(cm, annot=True, fmt=\"d\", cmap='Reds', xticklabels=classes, yticklabels=classes)\n",
        "        plt.ylabel('True label')\n",
        "        plt.xlabel('Predicted label')\n",
        "        plt.title('Confusion Matrix')\n",
        "        plt.show()\n",
        "\n",
        "def create_crtnet_original(number_of_leads=1, num_classes=5, multilabel=False, learning_rate=0.001):\n",
        "    tf.keras.backend.clear_session()\n",
        "    return crtnet_models.crt_net_original(\n",
        "        n_classes=num_classes,\n",
        "        input_shape=(None,number_of_leads),\n",
        "        n_vgg_blocks=5, # increased signal length so more CNN blocks to downsample (3000 / 2**5 -> 94)\n",
        "        binary=multilabel, # set this to true if using multilabel output (disables softmax and categorical cross entropy). CPSC can be multilabel.\n",
        "        use_focal=True, # addresses significant class imbalance (enables focal cross entropy)\n",
        "        metrics=['accuracy'], # May be better to evaluate on F1 score if using early stopping\n",
        "        d_model=128, # default feature dim size (d_ffn set to 2*d_model)\n",
        "        learning_rate=learning_rate\n",
        "    )\n",
        "\n",
        "def create_crtnet_alternate(number_of_leads=1, num_classes=5, multilabel=False, learning_rate=0.001):\n",
        "    tf.keras.backend.clear_session()\n",
        "    return crtnet_models.crt_net_original_alt(\n",
        "        n_classes=num_classes,\n",
        "        input_shape=(None,number_of_leads),\n",
        "        n_vgg_blocks=5, # increased signal length so more CNN blocks to downsample (3000 / 2**5 -> 94)\n",
        "        binary=multilabel, # set this to true if using multilabel output (disables softmax and categorical cross entropy). CPSC can be multilabel.\n",
        "        use_focal=True, # addresses significant class imbalance (enables focal cross entropy)\n",
        "        metrics=['accuracy'], # May be better to evaluate on F1 score if using early stopping\n",
        "        d_model=128, # default feature dim size (d_ffn set to 2*d_model)\n",
        "        learning_rate=learning_rate\n",
        "    )\n",
        "\n",
        "def create_crtnet_rwkv(number_of_leads=1, num_classes=5, multilabel=False, learning_rate=0.001):\n",
        "    tf.keras.backend.clear_session()\n",
        "    return crtnet_models.crt_net_modular(\n",
        "        n_classes=num_classes,\n",
        "        input_shape=(None,number_of_leads),\n",
        "        n_vgg_blocks=5, # increased signal length so more CNN blocks to downsample (3000 / 2**5 -> 94)\n",
        "        binary=multilabel, # set this to true if using multilabel output (disables softmax and categorical cross entropy). CPSC can be multilabel.\n",
        "        use_focal=True, # addresses significant class imbalance (enables focal cross entropy)\n",
        "        metrics=['accuracy'], # May be better to evaluate on F1 score if using early stopping\n",
        "        d_model=128, # default feature dim size (d_ffn set to 2*d_model)\n",
        "        att_type='rwkv',\n",
        "        learning_rate=learning_rate\n",
        "    )\n",
        "\n",
        "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
        "if len(physical_devices):\n",
        "    print(f'physical devices found: {physical_devices}')\n",
        "    mem_growth = tf.config.experimental.get_memory_growth(physical_devices[0])\n",
        "    print(f'memory growth of dev0: {mem_growth}')\n",
        "    if not mem_growth:\n",
        "        try:\n",
        "            tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
        "            print(f'memory growth of dev0: {tf.config.experimental.get_memory_growth(physical_devices[0])} (now enabled)')\n",
        "        except:\n",
        "            print(f'failed to modify device (likely already initialised)')\n",
        "else:\n",
        "    print('physical device not found')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Load data from numpy files, then train and evaluate of whichever model, by default the baseline CRT-Net\n",
        "\n",
        "The loss function depends on whether doing multilabel or single label classification, which is inferred from the data (ie if each sample has exlusive one hot encoding or multiple)\n",
        "\n",
        "The class names are read to label the outputs more clearly based on SNOMED, vs just having unlabeled one-hot encoding position"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "using the original/alternate architecture:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1715458575108
        }
      },
      "outputs": [],
      "source": [
        "import tensorflow.keras as keras\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import datetime\n",
        "import numpy as np\n",
        "    \n",
        "stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, min_lr=0.00001)\n",
        "\n",
        "# load the data from the file\n",
        "samples = np.load(os.path.join(\"data\", 'cpsc_2018_crtnet_samples.npy'))\n",
        "one_hot_encoding_labels = np.load(os.path.join(\"data\", 'cpsc_2018_crtnet_one_hot_encoding_labels.npy'))\n",
        "classes = np.load(os.path.join(\"data\", 'cpsc_2018_crtnet_classes.npy'))\n",
        "\n",
        "is_multilabel = any(sum(row) > 1 for row in one_hot_encoding_labels)\n",
        "\n",
        "use_alternate = True # if True: leaky ReLU (alpha=0.3); dropout (rate=0.2); max pos encoding = 2048; SeLU activation on pre-output\n",
        "initial_learning_rate = 0.0001\n",
        "\n",
        "create_crtnet_method = create_crtnet_alternate if use_alternate else create_crtnet_original\n",
        "model = create_crtnet_method(\n",
        "    number_of_leads=samples.shape[2],\n",
        "    num_classes=one_hot_encoding_labels.shape[1],\n",
        "    multilabel=is_multilabel,\n",
        "    learning_rate=initial_learning_rate)\n",
        "# model already compiled. it can still be recompiled if needed\n",
        "\n",
        "model.summary()\n",
        "\n",
        "train_and_evaluate_model(\n",
        "    model,\n",
        "    samples=samples,\n",
        "    one_hot_encoding_labels=one_hot_encoding_labels,\n",
        "    callbacks=[reduce_lr, stopping],\n",
        "    epochs=30,\n",
        "    batch_size=64,\n",
        "    classes=classes,\n",
        "    multilabel=is_multilabel,\n",
        "    style=[\"r--\", \"r--.\", \"b-\", \"b-*\", \"g-\", \"g-*\"] # has f1 score so add green line for this\n",
        ")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "tf215gpu"
    },
    "kernelspec": {
      "display_name": "tf215gpu",
      "language": "python",
      "name": "tf215gpu"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
