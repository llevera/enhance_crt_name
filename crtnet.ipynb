{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import numpy as np\n",
                "from sklearn.model_selection import train_test_split\n",
                "import wfdb\n",
                "import matplotlib.pyplot as plt\n",
                "import pandas as pd\n",
                "import seaborn as sns\n",
                "from sklearn.metrics import confusion_matrix\n",
                "\n",
                "def read_records(record_files):\n",
                "    records = []\n",
                "    labels = []\n",
                "    for record_file in record_files:\n",
                "        record = wfdb.rdrecord(record_file)\n",
                "        if record.file_name[0].endswith('.dat'):\n",
                "            # TODO work out how to deal with MIT-BIH with its different hea/atr/dat files\n",
                "            # and very low samples. Split into many files?\n",
                "            ann = wfdb.rdann(record_file,'atr')\n",
                "        else:\n",
                "            for comment in record.comments:\n",
                "                if comment.startswith('Dx') or comment.startswith(' Dx'):\n",
                "                    dxs = set(arr.strip() for arr in comment.split(': ')[1].split(','))\n",
                "                    labels.append(dxs)\n",
                "                    \n",
                "        records.append(wfdb.rdrecord(record_file))\n",
                "    return records, labels\n",
                "\n",
                "\n",
                "def create_one_hot_labels(all_labels, target_classes, num_recordings):\n",
                "    discard_index = list()\n",
                "    labels = np.zeros((num_recordings, len(target_classes)))#, dtype=np.bool)\n",
                "    for i in range(num_recordings):\n",
                "        dxs = all_labels[i]\n",
                "        flag = np.zeros((1,len(dxs)), dtype = bool)\n",
                "        count = 0\n",
                "        for dx in dxs:\n",
                "            if dx in target_classes:\n",
                "                j = target_classes.index(dx)\n",
                "                labels[i, j] = 1\n",
                "                flag [0 ,count] = True\n",
                "\n",
                "            count += 1\n",
                "\n",
                "    def __del__(self):\n",
                "        self.f.close()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 83,
            "metadata": {},
            "outputs": [],
            "source": [
                "import tensorflow as tf\n",
                "from tensorflow import keras\n",
                "import numpy as np\n",
                "import keras_nlp as knlp\n",
                "from tensorflow.keras.layers import LayerNormalization, Dropout, Dense, Add, Input\n",
                "\n",
                "def vgg_block(input, cnn_units):\n",
                "    output = keras.layers.Conv1D(cnn_units, 3, padding='same', activation='relu')(input)\n",
                "    output = keras.layers.BatchNormalization()(output)\n",
                "    output = keras.layers.Conv1D(cnn_units, 3, padding='same', activation='relu')(output)\n",
                "    output = keras.layers.BatchNormalization()(output)\n",
                "    output = keras.layers.MaxPooling1D(2, padding='same')(output)\n",
                "    return output\n",
                "\n",
                "def transformer_encoder_block(inputs, att_dim, att_heads, dropout_rate, ff_dim):\n",
                "    # Pre-Layer Normalization\n",
                "    norm_input = LayerNormalization()(inputs)\n",
                "    # Multi-head attention\n",
                "    attention_output = knlp.layers.TransformerEncoder(att_dim, att_heads, dropout=dropout_rate)(norm_input)\n",
                "    attention_output = Dropout(dropout_rate)(attention_output)\n",
                "    # Add & Norm\n",
                "    output = Add()([inputs, attention_output])\n",
                "    output = LayerNormalization()(output)\n",
                "    # Feed-forward\n",
                "    ff_output = Dense(ff_dim, activation='relu')(output)\n",
                "    ff_output = Dense(inputs.shape[-1])(ff_output)\n",
                "    ff_output = Dropout(dropout_rate)(ff_output)\n",
                "    # Add & Norm\n",
                "    output = Add()([output, ff_output])\n",
                "    return LayerNormalization()(output)\n",
                "\n",
                "\n",
                "def get_crt_model(n_classes, cnn_units=64, vgg_blocks=2, rnn_units=64, transformer_encoders=2, \n",
                "                  att_dim=64, att_heads=8, dropout_rate=0.2, fnn_units=64, ff_dim=256):\n",
                "    \n",
                "    input_layer = keras.Input(shape=(4096, 12))\n",
                "    output = input_layer\n",
                "\n",
                "    classes3 = list()\n",
                "    for x in classes2:\n",
                "        if x not in classes3:\n",
                "            classes3.append(x)\n",
                "\n",
                "    output = keras.layers.Bidirectional(keras.layers.GRU(rnn_units, return_sequences=True))(output)\n",
                "\n",
                "    if transformer_encoders > 0:\n",
                "        # Dynamic Position Encoding\n",
                "        dynamic_wavelength = compute_dynamic_wavelength(output.shape[1])  # ensure this function is defined\n",
                "        positional_encoding = knlp.layers.SinePositionEncoding(max_wavelength=dynamic_wavelength)(output)\n",
                "        output = Add()([output, positional_encoding])\n",
                "        output = LayerNormalization()(output)\n",
                "\n",
                "        for _ in range(transformer_encoders):\n",
                "            output = transformer_encoder_block(output, att_dim, att_heads, dropout_rate, ff_dim)\n",
                "\n",
                "        output = keras.layers.GlobalAveragePooling1D()(output)\n",
                "\n",
                "    output = Dropout(0.2)(output)\n",
                "    output = keras.layers.Dense(fnn_units, activation='relu')(output)\n",
                "    output = keras.layers.Dense(fnn_units // 2, activation='relu')(output)\n",
                "    output = keras.layers.Dense(n_classes, activation='softmax')(output)\n",
                "    \n",
                "    return keras.Model(inputs=input_layer, outputs=output)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 84,
            "metadata": {},
            "outputs": [],
            "source": [
                "val_split = 0.02\n",
                "lr = 0.0001\n",
                "batch_size = 32\n",
                "\n",
                "def train():\n",
                "    opt = tf.keras.optimizers.Adam(lr, clipnorm=1.0)\n",
                "    callbacks = [tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\n",
                "                                    factor=0.1,\n",
                "                                    patience=7,\n",
                "                                    min_lr=lr / 100),\n",
                "                    tf.keras.callbacks.EarlyStopping(patience=9,  # Patience should be larger than the one in ReduceLROnPlateau\n",
                "                                min_delta=0.00001)]\n",
                "\n",
                "filepath = os.path.join('output', \"model.h5\")\n",
                "\n",
                "    # If you are continuing an interrupted section, uncomment line bellow:\n",
                "    #model = keras.models.load_model(PATH_TO_PREV_MODEL, compile=False)\n",
                "    model = get_crt_model(train_seq.n_classes)\n",
                "    model.compile(loss=tf.keras.losses.CategoricalCrossentropy(), optimizer=opt)\n",
                "    # Create log\n",
                "    callbacks += [tf.keras.callbacks.TensorBoard(log_dir='./logs', write_graph=False),\n",
                "                    tf.keras.callbacks.CSVLogger('training.log', append=False)]  # Change append to true if continuing training\n",
                "    # Save the BEST and LAST model\n",
                "    #    callbacks += [tf.keras.callbacks.ModelCheckpoint('./backup_model_last.hdf5', overwrite=True),\n",
                "    #                  tf.keras.callbacks.ModelCheckpoint('./backup_model_best.hdf5', save_best_only=True, overwrite=True)]\n",
                "    # Train neural network\n",
                "    history = model.fit(train_seq, \n",
                "                        epochs=70,\n",
                "                        initial_epoch=0,  # If you are continuing a interrupted section change here\n",
                "                        #callbacks=callbacks,\n",
                "                        validation_data=valid_seq,\n",
                "                        verbose=1)\n",
                "    # Save final result\n",
                "    model.save(path_to_model)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 85,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch 1/70\n",
                        "26/26 [==============================] - 16s 313ms/step - loss: 0.3475 - val_loss: 0.1236\n",
                        "Epoch 2/70\n",
                        "26/26 [==============================] - 7s 276ms/step - loss: 0.3420 - val_loss: 0.1299\n",
                        "Epoch 3/70\n",
                        "26/26 [==============================] - 7s 271ms/step - loss: 0.3366 - val_loss: 0.1400\n",
                        "Epoch 4/70\n",
                        "26/26 [==============================] - 7s 278ms/step - loss: 0.3363 - val_loss: 0.1340\n",
                        "Epoch 5/70\n",
                        "26/26 [==============================] - 7s 274ms/step - loss: 0.3474 - val_loss: 0.1332\n",
                        "Epoch 6/70\n",
                        "26/26 [==============================] - 7s 279ms/step - loss: 0.3455 - val_loss: 0.1327\n",
                        "Epoch 7/70\n",
                        "26/26 [==============================] - 7s 278ms/step - loss: 0.3566 - val_loss: 0.1530\n",
                        "Epoch 8/70\n",
                        "26/26 [==============================] - 7s 278ms/step - loss: 0.3642 - val_loss: 0.1744\n",
                        "Epoch 9/70\n",
                        "26/26 [==============================] - 7s 283ms/step - loss: 0.3710 - val_loss: 0.1747\n",
                        "Epoch 10/70\n",
                        "26/26 [==============================] - 7s 274ms/step - loss: 0.3704 - val_loss: 0.1613\n",
                        "Epoch 11/70\n",
                        "26/26 [==============================] - 7s 277ms/step - loss: 0.3685 - val_loss: 0.1515\n",
                        "Epoch 12/70\n",
                        "26/26 [==============================] - 7s 278ms/step - loss: 0.3645 - val_loss: 0.1469\n",
                        "Epoch 13/70\n",
                        "26/26 [==============================] - 7s 277ms/step - loss: 0.3687 - val_loss: 0.1427\n",
                        "Epoch 14/70\n",
                        "26/26 [==============================] - 7s 276ms/step - loss: 0.3746 - val_loss: 0.1464\n",
                        "Epoch 15/70\n",
                        "26/26 [==============================] - 7s 280ms/step - loss: 0.3750 - val_loss: 0.1450\n",
                        "Epoch 16/70\n",
                        "26/26 [==============================] - 7s 281ms/step - loss: 0.3714 - val_loss: 0.1528\n",
                        "Epoch 17/70\n",
                        "26/26 [==============================] - 7s 286ms/step - loss: 0.3806 - val_loss: 0.1539\n",
                        "Epoch 18/70\n",
                        "26/26 [==============================] - 7s 280ms/step - loss: 0.3743 - val_loss: 0.1514\n",
                        "Epoch 19/70\n",
                        "26/26 [==============================] - 7s 282ms/step - loss: 0.3665 - val_loss: 0.1437\n",
                        "Epoch 20/70\n",
                        "26/26 [==============================] - 7s 278ms/step - loss: 0.3784 - val_loss: 0.1589\n",
                        "Epoch 21/70\n",
                        "26/26 [==============================] - 7s 275ms/step - loss: 0.3897 - val_loss: 0.1547\n",
                        "Epoch 22/70\n",
                        "26/26 [==============================] - 7s 280ms/step - loss: 0.3879 - val_loss: 0.1604\n",
                        "Epoch 23/70\n",
                        "26/26 [==============================] - 7s 271ms/step - loss: 0.3866 - val_loss: 0.1572\n",
                        "Epoch 24/70\n",
                        "26/26 [==============================] - 7s 279ms/step - loss: 0.3949 - val_loss: 0.1544\n",
                        "Epoch 25/70\n",
                        "26/26 [==============================] - 7s 275ms/step - loss: 0.3801 - val_loss: 0.1490\n",
                        "Epoch 26/70\n",
                        "26/26 [==============================] - 8s 293ms/step - loss: 0.3901 - val_loss: 0.1552\n",
                        "Epoch 27/70\n",
                        "26/26 [==============================] - 7s 282ms/step - loss: 0.3681 - val_loss: 0.1459\n",
                        "Epoch 28/70\n",
                        "26/26 [==============================] - 7s 284ms/step - loss: 0.3825 - val_loss: 0.1449\n",
                        "Epoch 29/70\n",
                        "26/26 [==============================] - 7s 281ms/step - loss: 0.3922 - val_loss: 0.1432\n",
                        "Epoch 30/70\n",
                        "26/26 [==============================] - 7s 284ms/step - loss: 0.3810 - val_loss: 0.1471\n",
                        "Epoch 31/70\n",
                        "26/26 [==============================] - 7s 275ms/step - loss: 0.3774 - val_loss: 0.1498\n",
                        "Epoch 32/70\n",
                        "26/26 [==============================] - 7s 274ms/step - loss: 0.3919 - val_loss: 0.1462\n",
                        "Epoch 33/70\n",
                        "26/26 [==============================] - 7s 276ms/step - loss: 0.3764 - val_loss: 0.1510\n",
                        "Epoch 34/70\n",
                        "26/26 [==============================] - 7s 282ms/step - loss: 0.3831 - val_loss: 0.1289\n",
                        "Epoch 35/70\n",
                        "26/26 [==============================] - 8s 295ms/step - loss: 0.3902 - val_loss: 0.1537\n",
                        "Epoch 36/70\n",
                        "26/26 [==============================] - 7s 284ms/step - loss: 0.3866 - val_loss: 0.1466\n",
                        "Epoch 37/70\n",
                        "26/26 [==============================] - 8s 299ms/step - loss: 0.3825 - val_loss: 0.1460\n",
                        "Epoch 38/70\n",
                        "26/26 [==============================] - 8s 293ms/step - loss: 0.3841 - val_loss: 0.1471\n",
                        "Epoch 39/70\n",
                        "26/26 [==============================] - 7s 274ms/step - loss: 0.3866 - val_loss: 0.1224\n",
                        "Epoch 40/70\n",
                        "26/26 [==============================] - 7s 273ms/step - loss: 0.3983 - val_loss: 0.1309\n",
                        "Epoch 41/70\n",
                        "26/26 [==============================] - 7s 277ms/step - loss: 0.3828 - val_loss: 0.1631\n",
                        "Epoch 42/70\n",
                        "26/26 [==============================] - 7s 270ms/step - loss: 0.3812 - val_loss: 0.1375\n",
                        "Epoch 43/70\n",
                        "26/26 [==============================] - 7s 283ms/step - loss: 0.3931 - val_loss: 0.1348\n",
                        "Epoch 44/70\n",
                        "26/26 [==============================] - 7s 271ms/step - loss: 0.3994 - val_loss: 0.1447\n",
                        "Epoch 45/70\n",
                        "26/26 [==============================] - 7s 283ms/step - loss: 0.3720 - val_loss: 0.1322\n",
                        "Epoch 46/70\n",
                        "26/26 [==============================] - 8s 288ms/step - loss: 0.3860 - val_loss: 0.1305\n",
                        "Epoch 47/70\n",
                        "26/26 [==============================] - 7s 282ms/step - loss: 0.3841 - val_loss: 0.1278\n",
                        "Epoch 48/70\n",
                        "26/26 [==============================] - 7s 280ms/step - loss: 0.3792 - val_loss: 0.1292\n",
                        "Epoch 49/70\n",
                        "26/26 [==============================] - 7s 278ms/step - loss: 0.3876 - val_loss: 0.1392\n",
                        "Epoch 50/70\n",
                        "26/26 [==============================] - 7s 280ms/step - loss: 0.3994 - val_loss: 0.1558\n",
                        "Epoch 51/70\n",
                        "26/26 [==============================] - 7s 281ms/step - loss: 0.3819 - val_loss: 0.1292\n",
                        "Epoch 52/70\n",
                        "26/26 [==============================] - 7s 284ms/step - loss: 0.3897 - val_loss: 0.1158\n",
                        "Epoch 53/70\n",
                        "26/26 [==============================] - 7s 279ms/step - loss: 0.3930 - val_loss: 0.1284\n",
                        "Epoch 54/70\n",
                        "26/26 [==============================] - 7s 285ms/step - loss: 0.3786 - val_loss: 0.1310\n",
                        "Epoch 55/70\n",
                        "26/26 [==============================] - 7s 281ms/step - loss: 0.3878 - val_loss: 0.1490\n",
                        "Epoch 56/70\n",
                        "26/26 [==============================] - 7s 282ms/step - loss: 0.3990 - val_loss: 0.1310\n",
                        "Epoch 57/70\n",
                        "26/26 [==============================] - 7s 285ms/step - loss: 0.3908 - val_loss: 0.1444\n",
                        "Epoch 58/70\n",
                        "26/26 [==============================] - 8s 290ms/step - loss: 0.3803 - val_loss: 0.1171\n",
                        "Epoch 59/70\n",
                        "26/26 [==============================] - 8s 290ms/step - loss: 0.3866 - val_loss: 0.1317\n",
                        "Epoch 60/70\n",
                        "26/26 [==============================] - 7s 281ms/step - loss: 0.3863 - val_loss: 0.1444\n",
                        "Epoch 61/70\n",
                        "26/26 [==============================] - 8s 289ms/step - loss: 0.3895 - val_loss: 0.1207\n",
                        "Epoch 62/70\n",
                        "26/26 [==============================] - 7s 286ms/step - loss: 0.3749 - val_loss: 0.1420\n",
                        "Epoch 63/70\n",
                        "26/26 [==============================] - 8s 289ms/step - loss: 0.3736 - val_loss: 0.1171\n",
                        "Epoch 64/70\n",
                        "26/26 [==============================] - 8s 290ms/step - loss: 0.3766 - val_loss: 0.1081\n",
                        "Epoch 65/70\n",
                        "26/26 [==============================] - 8s 292ms/step - loss: 0.3846 - val_loss: 0.1543\n",
                        "Epoch 66/70\n",
                        "26/26 [==============================] - 8s 288ms/step - loss: 0.3933 - val_loss: 0.1242\n",
                        "Epoch 67/70\n",
                        "26/26 [==============================] - 7s 288ms/step - loss: 0.3852 - val_loss: 0.1263\n",
                        "Epoch 68/70\n",
                        "26/26 [==============================] - 7s 282ms/step - loss: 0.3803 - val_loss: 0.1346\n",
                        "Epoch 69/70\n",
                        "26/26 [==============================] - 8s 291ms/step - loss: 0.3895 - val_loss: 0.1375\n",
                        "Epoch 70/70\n",
                        "26/26 [==============================] - 8s 293ms/step - loss: 0.3814 - val_loss: 0.1483\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, self_attention_layer_layer_call_fn while saving (showing 5 of 60). These functions will not be directly callable after loading.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "INFO:tensorflow:Assets written to: final_model.hf5\\assets\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "INFO:tensorflow:Assets written to: final_model.hf5\\assets\n"
                    ]
                }
            ],
            "source": [
                "train()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 86,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import warnings\n",
                "import argparse\n",
                "from tensorflow.keras.models import load_model\n",
                "from tensorflow.keras.optimizers import Adam\n",
                "\n",
                "output_file = \"out\"\n",
                "\n",
                "def evaluate():\n",
                "    seq = ECGSequence(path_to_hdf5, dataset_name, batch_size=batch_size)\n",
                "    # Import model\n",
                "    model = load_model(path_to_model, compile=False)\n",
                "    model.compile(loss='binary_crossentropy', optimizer=Adam())\n",
                "    y_score = model.predict(seq,  verbose=1)\n",
                "\n",
                "    # Generate dataframe\n",
                "    np.save(\"predict_outputs\", y_score)\n",
                "\n",
                "    print(\"Output predictions saved\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 87,
            "metadata": {
                "scrolled": true
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "26/26 [==============================] - 4s 93ms/step\n",
                        "Output predictions saved\n"
                    ]
                }
            ],
            "source": [
                "evaluate()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 88,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[[0.09314 0.09314 0.2532  0.2532  0.1536  0.1536 ]\n",
                        " [0.09314 0.09314 0.2532  0.2532  0.1536  0.1536 ]\n",
                        " [0.09314 0.09314 0.2532  0.2532  0.1536  0.1536 ]\n",
                        " ...\n",
                        " [0.09314 0.09314 0.2532  0.2532  0.1536  0.1536 ]\n",
                        " [0.09314 0.09314 0.2532  0.2532  0.1536  0.1536 ]\n",
                        " [0.09314 0.09314 0.2532  0.2532  0.1536  0.1536 ]]\n"
                    ]
                }
            ],
            "source": [
                "data = np.load('predict_outputs.npy')\n",
                "print(data)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.19"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
