{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import wfdb\n",
    "\n",
    "def read_records(record_files):\n",
    "    records = []\n",
    "    labels = []\n",
    "    for record_file in record_files:\n",
    "        record = wfdb.rdrecord(record_file)\n",
    "        if record.file_name[0].endswith('.dat'):\n",
    "            # TODO work out how to deal with MIT-BIH with its different hea/atr/dat files\n",
    "            # and very low samples. Split into many files?\n",
    "            ann = wfdb.rdann(record_file,'atr')\n",
    "        else:\n",
    "            for comment in record.comments:\n",
    "                if comment.startswith('Dx') or comment.startswith(' Dx'):\n",
    "                    dxs = set(arr.strip() for arr in comment.split(': ')[1].split(','))\n",
    "                    labels.append(dxs)\n",
    "                else:\n",
    "                    labels.append(set())\n",
    "        records.append(wfdb.rdrecord(record_file))\n",
    "    return records, labels\n",
    "\n",
    "\n",
    "def create_one_hot_labels(all_labels, unique_classes, num_recordings):\n",
    "    index = list()\n",
    "    labels = np.zeros((num_recordings, len(unique_classes)))#, dtype=np.bool)\n",
    "    for i in range(num_recordings):\n",
    "        dxs = all_labels[i]\n",
    "        flag = np.zeros((1,len(dxs)), dtype = bool)\n",
    "        count = 0\n",
    "        for dx in dxs:\n",
    "            if dx in unique_classes:\n",
    "                j = unique_classes.index(dx)\n",
    "                labels[i, j] = 1\n",
    "                flag [0 ,count] = True\n",
    "\n",
    "            count += 1\n",
    "\n",
    "        if np.any(flag) == False:\n",
    "            index.append(i)\n",
    "\n",
    "    return labels, index\n",
    "\n",
    "def get_unique_classes(all_labels, valid_classes=None):\n",
    "\n",
    "    classes2 = list()\n",
    "    for i in range(len(all_labels)):\n",
    "        dxs = all_labels[i]\n",
    "        for dx in dxs:\n",
    "            if valid_classes is None or dx in valid_classes:\n",
    "                classes2.append(dx)\n",
    "\n",
    "    classes3 = list()\n",
    "    for x in classes2:\n",
    "        if x not in classes3:\n",
    "            classes3.append(x)\n",
    "\n",
    "    classes3 = sorted (classes3)\n",
    "    return classes3\n",
    "\n",
    "def find_records(directory):\n",
    "    record_files = []\n",
    "    for dirpath, _, filenames in os.walk(directory):\n",
    "        for f in sorted(filenames):\n",
    "            file_path = os.path.join(dirpath, f)\n",
    "            if os.path.isfile(file_path) and not f.lower().startswith('.'):\n",
    "                file, ext = os.path.splitext(file_path)\n",
    "                if ext.lower() == '.hea':\n",
    "                    record_files.append(file)\n",
    "    if record_files:\n",
    "        return record_files\n",
    "    else:\n",
    "        raise IOError('No record files found.')\n",
    "\n",
    "def filter(data, labels, index):\n",
    "    labels = [labels[i] for i in range(len(labels)) if i not in index]\n",
    "    data = [data[i] for i in range(len(data)) if i not in index]\n",
    "    return labels, data\n",
    "\n",
    "def consolidate_equivalent_classes(labels, unique_classes):\n",
    "    equivalent_classes_collection = [['713427006', '59118001'], ['284470004', '63593006'], ['427172004', '17338001']]\n",
    "\n",
    "    # For each set of equivalent class, use only one class as the representative class for the set and discard the other classes in the set.\n",
    "    # The label for the representative class is positive if any of the labels in the set is positive.\n",
    "    remove_classes = list()\n",
    "    remove_indices = list()\n",
    "    for equivalent_classes in equivalent_classes_collection:\n",
    "        equivalent_classes = [x for x in equivalent_classes if x in unique_classes]\n",
    "        if len(equivalent_classes)>1:\n",
    "            other_classes = equivalent_classes[1:]\n",
    "            equivalent_indices = [unique_classes.index(x) for x in equivalent_classes]\n",
    "            representative_index = equivalent_indices[0]\n",
    "            other_indices = equivalent_indices[1:]\n",
    "\n",
    "            labels[:, representative_index] = np.any(labels[:, equivalent_indices], axis=1)\n",
    "            remove_classes += other_classes\n",
    "            remove_indices += other_indices\n",
    "\n",
    "    for x in remove_classes:\n",
    "        unique_classes.remove(x)\n",
    "    labels = np.delete(labels, remove_indices, axis=1)\n",
    "\n",
    "    return labels, unique_classes\n",
    "\n",
    "def set_labels_to_normal_if_none_other(labels, unique_classes, normal_class):\n",
    "    # If the labels are negative for all classes, then change the label for the normal class to positive.\n",
    "    normal_index = unique_classes.index(normal_class)\n",
    "    for i in range(len(labels)):\n",
    "        num_positive_classes = np.sum(labels[i, :])\n",
    "        if num_positive_classes==0:\n",
    "            labels[i, normal_index] = 1\n",
    "\n",
    "    return labels\n",
    "\n",
    "def ensure_normal_class(unique_classes, normal_class):\n",
    "    if normal_class not in unique_classes:\n",
    "        unique_classes.add(normal_class)\n",
    "        print('- The normal class {} is not one of the label classes, so it has been automatically added, but please check that you chose the correct normal class.'.format(normal_class))\n",
    "    unique_classes = sorted(unique_classes)\n",
    "    return unique_classes\n",
    "\n",
    "def read_scored_classes():\n",
    "    scored = list()\n",
    "    with open('dx_mapping_scored.csv', 'r') as f:\n",
    "        for l in f:\n",
    "            dxs = (l.split(','))\n",
    "            scored.append(dxs[1])\n",
    "    return (sorted(scored[1:]))\n",
    "\n",
    "def filter(labels, records, keep_index):\n",
    "    labels = [labels[i] for i in range(len(labels)) if i not in keep_index]\n",
    "    records = [records[i] for i in range(len(records)) if i not in keep_index]\n",
    "\n",
    "    return labels, records\n",
    "\n",
    "def load_records(header_file_list, adjust_classes_for_physionet, normal_class=None):\n",
    "        \n",
    "    if len(header_file_list) == 0:\n",
    "        raise ValueError('No header files found.')\n",
    "\n",
    "    num_recordings = len(header_file_list)\n",
    "\n",
    "    records, all_labels = read_records(header_file_list)\n",
    "\n",
    "    scored = None\n",
    "    if adjust_classes_for_physionet:\n",
    "        scored = read_scored_classes()\n",
    "\n",
    "    unique_classes = get_unique_classes(all_labels, scored)\n",
    "\n",
    "    if (normal_class is not None):\n",
    "        unique_classes = ensure_normal_class(unique_classes, normal_class)\n",
    "    \n",
    "    labels, keep_index = create_one_hot_labels(all_labels, unique_classes, num_recordings)\n",
    "\n",
    "    if (adjust_classes_for_physionet):\n",
    "        labels, unique_classes = consolidate_equivalent_classes(labels, unique_classes)\n",
    "    \n",
    "    if (normal_class is not None):\n",
    "        labels = set_labels_to_normal_if_none_other(labels, unique_classes, normal_class)\n",
    "    \n",
    "    labels, records = filter(labels, records, keep_index)\n",
    "\n",
    "    return labels, records\n",
    "\n",
    "def standardise_length(data, target_length):\n",
    "    number_of_leads = data.shape[0]\n",
    "    \n",
    "    if len(data[0])<=target_length:\n",
    "        ext= np.zeros([number_of_leads,target_length])\n",
    "        for i in range(0,number_of_leads):\n",
    "            ext[i][0:len(data[i])]=data[i]\n",
    "        return ext.T  \n",
    "    else:\n",
    "        cut = np.zeros([number_of_leads,target_length])\n",
    "        for i in range(number_of_leads):\n",
    "            tocut = len(data[0])- target_length\n",
    "            cut[i] = data[i][tocut:]\n",
    "        return cut.T \n",
    "\n",
    "def resample(data, src_frq, trg_frq):\n",
    "\n",
    "    if src_frq == trg_frq:\n",
    "        return data\n",
    "\n",
    "    N_src = data.shape[0]\n",
    "    N_trg = int(N_src * trg_frq / src_frq)\n",
    "    \n",
    "    resampled = np.zeros((N_trg, data.shape[1]), dtype='float32')\n",
    "    for i in range(data.shape[1]):\n",
    "        resampled[:,i] = np.interp(np.linspace(0, N_src, N_trg), np.arange(N_src), data[:, i])\n",
    "        \n",
    "    return resampled\n",
    "\n",
    "def standardise_data_samples(records):\n",
    "    standardised_data = list()\n",
    "\n",
    "    # find the most common fs\n",
    "    fss = [record.fs for record in records]\n",
    "    target_fs = max(set(fss), key=fss.count)\n",
    "\n",
    "    # find the most common sig_len\n",
    "    sig_lens = [record.sig_len for record in records]\n",
    "    target_length = max(set(sig_lens), key=sig_lens.count)\n",
    "\n",
    "    for i in range(len(records)):\n",
    "        datum = records[i].p_signal.T\n",
    "        datum = resample(datum,records[i].fs, target_fs)\n",
    "        datum = standardise_length(datum, target_length)\n",
    "        standardised_data.append(datum)\n",
    "\n",
    "    return standardised_data\n",
    "\n",
    "def load_data(input_directory, adjust_classes_for_physionet, normal_class):\n",
    "    record_file_list = find_records(input_directory)\n",
    "    \n",
    "    labels, records = load_records(record_file_list, adjust_classes_for_physionet, normal_class=normal_class)\n",
    "    samples = standardise_data_samples(records)\n",
    "            \n",
    "    labels = np.stack(labels, axis =0)\n",
    "    samples = np.stack(samples, axis =0)\n",
    "\n",
    "    train_x, test_x, train_y, test_y = train_test_split(samples, labels, test_size=0.1, random_state=42)\n",
    "    train_x, val_x, train_y, val_y = train_test_split(train_x, train_y, test_size=0.1, random_state=42)\n",
    "    \n",
    "    return train_x,train_y,val_x,val_y, test_x, test_y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "import keras_nlp as nlp \n",
    "import tensorflow.keras as keras\n",
    "\n",
    "def vgg_block(input, cnn_units):\n",
    "    output = input\n",
    "    output = layers.Conv1D(cnn_units, 3, padding='same', activation='relu')(output)\n",
    "    output = layers.Conv1D(cnn_units, 3, padding='same', activation='relu')(output)\n",
    "    output = layers.MaxPooling1D(2, padding='same')(output)\n",
    "    return output\n",
    "\n",
    "def crt_net(\n",
    "        number_of_leads,\n",
    "        cnn_units=128,\n",
    "        vgg_blocks=1,\n",
    "        rnn_units=64,\n",
    "        transformer_encoders=4,\n",
    "        att_dim=64,\n",
    "        att_heads=8,\n",
    "        fnn_units=64,\n",
    "        num_classes=6\n",
    "    ):\n",
    "    input = layers.Input(shape=(None, number_of_leads))\n",
    "    output = input\n",
    "\n",
    "    for _ in range(vgg_blocks):\n",
    "        output = vgg_block(output, cnn_units)\n",
    "\n",
    "    output = layers.Bidirectional(layers.GRU(rnn_units, return_sequences=True), merge_mode='sum')(output)\n",
    "\n",
    "    if transformer_encoders > 0:\n",
    "        output = output + nlp.layers.SinePositionEncoding(max_wavelength=10000)(output)\n",
    "\n",
    "        for _ in range(transformer_encoders):\n",
    "            output = nlp.layers.TransformerEncoder(att_dim, att_heads)(output)\n",
    "\n",
    "        output = layers.GlobalAveragePooling1D()(output)\n",
    "        \n",
    "    output = layers.Dropout(0.2)(output)\n",
    "    output = layers.Dense(fnn_units, activation='relu')(output)\n",
    "    output = layers.Dense(fnn_units, activation='relu')(output)\n",
    "\n",
    "    output = layers.Dense(num_classes, activation='sigmoid')(output)\n",
    "    return keras.Model(input, output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Epoch 1/2\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.6657 - accuracy: 0.1111  "
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unable to create dataset (name already exists)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[124], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m model \u001b[38;5;241m=\u001b[39m crt_net(number_of_leads\u001b[38;5;241m=\u001b[39mtrain_x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m], num_classes\u001b[38;5;241m=\u001b[39mtrain_y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])  \n\u001b[1;32m     26\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m\"\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m\"\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m---> 28\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMAX_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mval_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_y\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce_lr\u001b[49m\u001b[43m,\u001b[49m\u001b[43mstopping\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m pd\u001b[38;5;241m.\u001b[39mDataFrame(history\u001b[38;5;241m.\u001b[39mhistory)\u001b[38;5;241m.\u001b[39mplot(\n\u001b[1;32m     36\u001b[0m     figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m5\u001b[39m), xlim\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m100\u001b[39m], ylim\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m], grid\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, xlabel\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     37\u001b[0m     style\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr--\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr--.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb-\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb-*\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     38\u001b[0m plt\u001b[38;5;241m.\u001b[39mlegend(loc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlower left\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow210auto/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow210auto/lib/python3.10/site-packages/h5py/_hl/group.py:183\u001b[0m, in \u001b[0;36mGroup.create_dataset\u001b[0;34m(self, name, shape, dtype, data, **kwds)\u001b[0m\n\u001b[1;32m    180\u001b[0m         parent_path, name \u001b[38;5;241m=\u001b[39m name\u001b[38;5;241m.\u001b[39mrsplit(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    181\u001b[0m         group \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequire_group(parent_path)\n\u001b[0;32m--> 183\u001b[0m dsid \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_new_dset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m dset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mDataset(dsid)\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dset\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow210auto/lib/python3.10/site-packages/h5py/_hl/dataset.py:163\u001b[0m, in \u001b[0;36mmake_new_dset\u001b[0;34m(parent, shape, dtype, data, name, chunks, compression, shuffle, fletcher32, maxshape, compression_opts, fillvalue, scaleoffset, track_times, external, track_order, dcpl, dapl, efile_prefix, virtual_prefix, allow_unknown_filter, rdcc_nslots, rdcc_nbytes, rdcc_w0)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    161\u001b[0m     sid \u001b[38;5;241m=\u001b[39m h5s\u001b[38;5;241m.\u001b[39mcreate_simple(shape, maxshape)\n\u001b[0;32m--> 163\u001b[0m dset_id \u001b[38;5;241m=\u001b[39m \u001b[43mh5d\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdcpl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdcpl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdapl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdapl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, Empty)):\n\u001b[1;32m    166\u001b[0m     dset_id\u001b[38;5;241m.\u001b[39mwrite(h5s\u001b[38;5;241m.\u001b[39mALL, h5s\u001b[38;5;241m.\u001b[39mALL, data)\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5d.pyx:138\u001b[0m, in \u001b[0;36mh5py.h5d.create\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unable to create dataset (name already exists)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "output_directory = 'output'\n",
    "\n",
    "input_directory = 'training_data/cpsc_2018_subset'\n",
    "adjust_classes_for_physionet = False\n",
    "normal_class = None #'426783006'\n",
    "\n",
    "MAX_EPOCHS = 2\n",
    "    \n",
    "stopping = keras.callbacks.EarlyStopping(patience=5)\n",
    "\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "    factor=0.1,\n",
    "    patience=3,\n",
    "    min_lr= 0.001*0.001)\n",
    "\n",
    "filepath = os.path.join('output', \"model.h5\")\n",
    "\n",
    "#checkpoint = keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "        \n",
    "train_x, train_y, val_x, val_y, test_x, test_y = load_data(input_directory, adjust_classes_for_physionet=False, normal_class=normal_class)\n",
    "        \n",
    "model = crt_net(number_of_leads=train_x.shape[2], num_classes=train_y.shape[1])  \n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    \n",
    "history = model.fit(\n",
    "    train_x, train_y,\n",
    "    batch_size=32,\n",
    "    epochs=MAX_EPOCHS,\n",
    "    validation_data=(val_x, val_y),\n",
    "    callbacks= [reduce_lr,stopping])\n",
    "\n",
    "pd.DataFrame(history.history).plot(\n",
    "    figsize=(8, 5), xlim=[0, 100], ylim=[0, 1], grid=True, xlabel=\"Epoch\",\n",
    "    style=[\"r--\", \"r--.\", \"b-\", \"b-*\"])\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
